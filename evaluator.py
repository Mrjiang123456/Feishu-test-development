import json
import aiohttp
from logger import log, log_error, end_logging
from llm_api import async_call_llm, extract_valid_json
from analyzer import find_duplicate_test_cases
import re
import asyncio
import concurrent.futures
from config import MAX_CONCURRENT_REQUESTS, LLM_TEMPERATURE, LLM_TEMPERATURE_REPORT, ENABLE_MULTI_JUDGES, ENABLE_COLLAB_EVAL
from typing import Dict
import time
import uuid
import os

# åœ¨æ–‡ä»¶é¡¶éƒ¨å¯¼å…¥å§”å‘˜ä¼šè¯„æµ‹åŠŸèƒ½
try:
    from committee import evaluate_with_committee

    COMMITTEE_IMPORTED = True
except ImportError:
    log_error("æ— æ³•å¯¼å…¥è¯„å§”å§”å‘˜ä¼šæ¨¡å—ï¼Œå°†ä½¿ç”¨å•ä¸€æ¨¡å‹è¯„æµ‹", level="WARNING")
    COMMITTEE_IMPORTED = False


async def evaluate_test_cases(session: aiohttp.ClientSession, ai_cases, golden_cases, is_iteration=False, prev_iteration_cases=None):
    """
    è¯„æµ‹æµ‹è¯•ç”¨ä¾‹è´¨é‡

    :param session: aiohttpä¼šè¯
    :param ai_cases: AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
    :param golden_cases: é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
    :param is_iteration: æ˜¯å¦å¯ç”¨è¿­ä»£å‰åå¯¹æ¯”åŠŸèƒ½
    :param prev_iteration_cases: ä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯é€‰ï¼‰ï¼Œä»…åœ¨is_iterationä¸ºtrueæ—¶æœ‰æ•ˆ
    :return: è¯„æµ‹ç»“æœ
    """
    log("å¼€å§‹æµ‹è¯•ç”¨ä¾‹è¯„æµ‹", important=True)
    if is_iteration and prev_iteration_cases:
        log("å¯ç”¨è¿­ä»£å‰åå¯¹æ¯”åŠŸèƒ½ï¼Œå°†åˆ†ææµ‹è¯•ç”¨ä¾‹è¿­ä»£æ”¹è¿›æƒ…å†µ", important=True)

    # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
    await asyncio.sleep(0.05)

    # è·å–æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    ai_testcases = []
    golden_testcases = []
    prev_testcases = []

    # å®šä¹‰æ ¼å¼åŒ–å‡½æ•°ï¼Œä¾¿äºå¹¶è¡Œå¤„ç†
    def extract_ai_testcases(ai_cases):
        result = []
        # æå–AIæµ‹è¯•ç”¨ä¾‹ï¼Œé€‚é…æ–°çš„æ ¼å¼åŒ–ç»“æ„
        if isinstance(ai_cases, dict):
            if "testcases" in ai_cases and isinstance(ai_cases["testcases"], dict):
                # æ–°çš„ç»Ÿä¸€æ ¼å¼
                if "test_cases" in ai_cases["testcases"]:
                    # å¤„ç†test_caseså¯èƒ½æ˜¯å­—å…¸(åŒ…å«ä¸åŒç±»åˆ«çš„æµ‹è¯•ç”¨ä¾‹)çš„æƒ…å†µ
                    if isinstance(ai_cases["testcases"]["test_cases"], dict):
                        for category, cases in ai_cases["testcases"]["test_cases"].items():
                            if isinstance(cases, list):
                                for case in cases:
                                    if isinstance(case, dict):
                                        case["category"] = category
                                    result.append(case)
                    # å¤„ç†test_casesæ˜¯åˆ—è¡¨çš„æƒ…å†µ
                    elif isinstance(ai_cases["testcases"]["test_cases"], list):
                        result = ai_cases["testcases"]["test_cases"]
            elif "test_cases" in ai_cases:
                if isinstance(ai_cases["test_cases"], dict):
                    # æ—§æ ¼å¼ï¼Œåˆ†ç±»æµ‹è¯•ç”¨ä¾‹
                    for category, cases in ai_cases["test_cases"].items():
                        if isinstance(cases, list):
                            result.extend(cases)
                elif isinstance(ai_cases["test_cases"], list):
                    # æ—§æ ¼å¼ï¼Œç›´æ¥åˆ—è¡¨
                    result = ai_cases["test_cases"]
        return result

    def extract_golden_testcases(golden_cases):
        result = []
        # æå–é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹ï¼Œé€‚é…æ–°çš„æ ¼å¼åŒ–ç»“æ„
        if isinstance(golden_cases, dict):
            if "testcases" in golden_cases and isinstance(golden_cases["testcases"], dict):
                # æ–°çš„ç»Ÿä¸€æ ¼å¼
                if "test_cases" in golden_cases["testcases"]:
                    # å¤„ç†test_caseså¯èƒ½æ˜¯å­—å…¸(åŒ…å«ä¸åŒç±»åˆ«çš„æµ‹è¯•ç”¨ä¾‹)çš„æƒ…å†µ
                    if isinstance(golden_cases["testcases"]["test_cases"], dict):
                        for category, cases in golden_cases["testcases"]["test_cases"].items():
                            if isinstance(cases, list):
                                for case in cases:
                                    if isinstance(case, dict):
                                        case["category"] = category
                                    result.append(case)
                    # å¤„ç†test_casesæ˜¯åˆ—è¡¨çš„æƒ…å†µ
                    elif isinstance(golden_cases["testcases"]["test_cases"], list):
                        result = golden_cases["testcases"]["test_cases"]
            elif "test_cases" in golden_cases:
                if isinstance(golden_cases["test_cases"], dict):
                    # æ—§æ ¼å¼ï¼Œåˆ†ç±»æµ‹è¯•ç”¨ä¾‹
                    for category, cases in golden_cases["test_cases"].items():
                        if isinstance(cases, list):
                            result.extend(cases)
                elif isinstance(golden_cases["test_cases"], list):
                    # æ—§æ ¼å¼ï¼Œç›´æ¥åˆ—è¡¨
                    result = golden_cases["test_cases"]
        return result
        
    # æå–ä¸Šä¸€æ¬¡è¿­ä»£æµ‹è¯•ç”¨ä¾‹
    def extract_prev_testcases(prev_cases):
        # å¤ç”¨æå–AIæµ‹è¯•ç”¨ä¾‹çš„é€»è¾‘
        return extract_ai_testcases(prev_cases)

    # å¹¶è¡Œæ‰§è¡ŒAIæµ‹è¯•ç”¨ä¾‹å’Œé»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹çš„æ ¼å¼åŒ–å¤„ç†
    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
        ai_future = executor.submit(extract_ai_testcases, ai_cases)
        golden_future = executor.submit(extract_golden_testcases, golden_cases)
        
        if is_iteration and prev_iteration_cases:
            prev_future = executor.submit(extract_prev_testcases, prev_iteration_cases)
            prev_testcases = prev_future.result()

        ai_testcases = ai_future.result()
        golden_testcases = golden_future.result()

    log(f"AIæµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(ai_testcases)}, é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(golden_testcases)}", important=True)
    if is_iteration and prev_testcases:
        log(f"ä¸Šä¸€æ¬¡è¿­ä»£æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(prev_testcases)}", important=True)

    # æ£€æŸ¥é‡å¤çš„æµ‹è¯•ç”¨ä¾‹
    ai_duplicate_info = find_duplicate_test_cases(ai_testcases)
    golden_duplicate_info = find_duplicate_test_cases(golden_testcases)
    
    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œä¹Ÿæ£€æŸ¥ä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹é‡å¤æƒ…å†µ
    if is_iteration and prev_testcases:
        prev_duplicate_info = find_duplicate_test_cases(prev_testcases)
        log(f"ä¸Šä¸€æ¬¡è¿­ä»£æµ‹è¯•ç”¨ä¾‹é‡å¤ç‡: {prev_duplicate_info['duplicate_rate']}% ({prev_duplicate_info['duplicate_count']}ä¸ª)",
            important=True)

    log(f"AIæµ‹è¯•ç”¨ä¾‹é‡å¤ç‡: {ai_duplicate_info['duplicate_rate']}% ({ai_duplicate_info['duplicate_count']}ä¸ª)",
        important=True)
    log(f"é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹é‡å¤ç‡: {golden_duplicate_info['duplicate_rate']}% ({golden_duplicate_info['duplicate_count']}ä¸ª)",
        important=True)

    # è®°å½•é‡å¤ç±»å‹åˆ†å¸ƒ
    log(f"AIæµ‹è¯•ç”¨ä¾‹é‡å¤ç±»å‹åˆ†å¸ƒ: {json.dumps(ai_duplicate_info['duplicate_types'], ensure_ascii=False)}",
        important=True)
    if ai_duplicate_info['duplicate_categories']:
        log(f"AIæµ‹è¯•ç”¨ä¾‹æŒ‰ç±»åˆ«é‡å¤ç‡: {json.dumps(ai_duplicate_info['duplicate_categories'], ensure_ascii=False)}",
            important=True)

    # æå–åˆå¹¶å»ºè®®
    merge_suggestions_count = len(ai_duplicate_info.get("merge_suggestions", []))
    log(f"ç”Ÿæˆäº† {merge_suggestions_count} æ¡AIæµ‹è¯•ç”¨ä¾‹åˆå¹¶å»ºè®®", important=True)

    # æ„å»ºè¯„æµ‹æç¤º
    duplicate_info_text = f"""
# æµ‹è¯•ç”¨ä¾‹é‡å¤æƒ…å†µ
## AIæµ‹è¯•ç”¨ä¾‹é‡å¤æƒ…å†µ
- é‡å¤ç‡: {ai_duplicate_info['duplicate_rate']}%
- é‡å¤æµ‹è¯•ç”¨ä¾‹æ•°é‡: {ai_duplicate_info['duplicate_count']}ä¸ª
- æ ‡é¢˜é‡å¤çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(ai_duplicate_info['title_duplicates'])}ä¸ª
- æ­¥éª¤é«˜åº¦ç›¸ä¼¼çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(ai_duplicate_info['steps_duplicates'])}ä¸ª
- åˆå¹¶å»ºè®®æ•°é‡: {merge_suggestions_count}ä¸ª

## é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹é‡å¤æƒ…å†µ
- é‡å¤ç‡: {golden_duplicate_info['duplicate_rate']}%
- é‡å¤æµ‹è¯•ç”¨ä¾‹æ•°é‡: {golden_duplicate_info['duplicate_count']}ä¸ª
- æ ‡é¢˜é‡å¤çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(golden_duplicate_info['title_duplicates'])}ä¸ª
- æ­¥éª¤é«˜åº¦ç›¸ä¼¼çš„æµ‹è¯•ç”¨ä¾‹æ•°é‡: {len(golden_duplicate_info['steps_duplicates'])}ä¸ª

å¦‚æœAIæµ‹è¯•ç”¨ä¾‹çš„é‡å¤ç‡æ˜æ˜¾é«˜äºé»„é‡‘æ ‡å‡†ï¼Œè¯·åœ¨æ”¹è¿›å»ºè®®ä¸­æå‡ºå‡å°‘é‡å¤æµ‹è¯•ç”¨ä¾‹çš„å»ºè®®ã€‚
"""

    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œæ·»åŠ è¿­ä»£å¯¹æ¯”ä¿¡æ¯
    iteration_comparison_text = ""
    if is_iteration and prev_testcases:
        # åˆ†æè¿­ä»£å‰åçš„æµ‹è¯•ç”¨ä¾‹å˜åŒ–
        prev_count = len(prev_testcases)
        current_count = len(ai_testcases)
        count_change = current_count - prev_count
        count_change_percent = (count_change / prev_count * 100) if prev_count > 0 else 0
        
        # åˆ†æé‡å¤ç‡å˜åŒ–
        prev_duplicate_rate = prev_duplicate_info['duplicate_rate']
        current_duplicate_rate = ai_duplicate_info['duplicate_rate']
        duplicate_rate_change = current_duplicate_rate - prev_duplicate_rate
        
        # è®¡ç®—åŠŸèƒ½è¦†ç›–å˜åŒ–ï¼ˆé€šè¿‡åˆ†ç±»æˆ–æ ‡é¢˜åˆ†æï¼‰
        prev_categories = set()
        current_categories = set()
        
        for case in prev_testcases:
            category = case.get("category", "")
            if category:
                prev_categories.add(category)
            # ä»æ ‡é¢˜ä¸­æå–åŠŸèƒ½ç‚¹
            title = case.get("title", "")
            if "åŠŸèƒ½" in title:
                feature = title.split("åŠŸèƒ½")[0] + "åŠŸèƒ½"
                prev_categories.add(feature)
            elif "æµ‹è¯•" in title:
                feature = title.split("æµ‹è¯•")[0]
                prev_categories.add(feature)
        
        for case in ai_testcases:
            category = case.get("category", "")
            if category:
                current_categories.add(category)
            # ä»æ ‡é¢˜ä¸­æå–åŠŸèƒ½ç‚¹
            title = case.get("title", "")
            if "åŠŸèƒ½" in title:
                feature = title.split("åŠŸèƒ½")[0] + "åŠŸèƒ½"
                current_categories.add(feature)
            elif "æµ‹è¯•" in title:
                feature = title.split("æµ‹è¯•")[0]
                current_categories.add(feature)
        
        # æ–°å¢åŠŸèƒ½ç‚¹
        new_categories = current_categories - prev_categories
        # ç§»é™¤çš„åŠŸèƒ½ç‚¹
        removed_categories = prev_categories - current_categories
        
        # æ„å»ºè¿­ä»£å¯¹æ¯”ä¿¡æ¯
        iteration_comparison_text = f"""
# è¿­ä»£å‰åå¯¹æ¯”åˆ†æ
## æµ‹è¯•ç”¨ä¾‹æ•°é‡å˜åŒ–
- ä¸Šä¸€æ¬¡è¿­ä»£: {prev_count}ä¸ªæµ‹è¯•ç”¨ä¾‹
- å½“å‰è¿­ä»£: {current_count}ä¸ªæµ‹è¯•ç”¨ä¾‹
- å˜åŒ–é‡: {count_change}ä¸ªæµ‹è¯•ç”¨ä¾‹ ({count_change_percent:.2f}%)

## é‡å¤ç‡å˜åŒ–
- ä¸Šä¸€æ¬¡è¿­ä»£é‡å¤ç‡: {prev_duplicate_rate}%
- å½“å‰è¿­ä»£é‡å¤ç‡: {current_duplicate_rate}%
- å˜åŒ–é‡: {duplicate_rate_change:.2f}ä¸ªç™¾åˆ†ç‚¹

## åŠŸèƒ½è¦†ç›–å˜åŒ–
- æ–°å¢åŠŸèƒ½ç‚¹: {len(new_categories)}ä¸ª
- ç§»é™¤åŠŸèƒ½ç‚¹: {len(removed_categories)}ä¸ª
"""
        
        # æ·»åŠ æ–°å¢å’Œç§»é™¤çš„åŠŸèƒ½ç‚¹è¯¦æƒ…
        if new_categories:
            iteration_comparison_text += "\n### æ–°å¢åŠŸèƒ½ç‚¹\n"
            for category in new_categories:
                iteration_comparison_text += f"- {category}\n"
        
        if removed_categories:
            iteration_comparison_text += "\n### ç§»é™¤åŠŸèƒ½ç‚¹\n"
            for category in removed_categories:
                iteration_comparison_text += f"- {category}\n"
                
        # æ·»åŠ å…·ä½“ç”¨ä¾‹å·®å¼‚åˆ†æ
        iteration_comparison_text += "\n## æµ‹è¯•ç”¨ä¾‹è´¨é‡å˜åŒ–åˆ†æ\n"
        
        # å¯¹æ¯”å…·ä½“æµ‹è¯•ç”¨ä¾‹å±æ€§ï¼Œå¦‚æ­¥éª¤æ•°é‡ã€é¢„æœŸç»“æœæ•°é‡ç­‰
        prev_avg_steps = sum(len(case.get("steps", [])) for case in prev_testcases) / prev_count if prev_count > 0 else 0
        current_avg_steps = sum(len(case.get("steps", [])) for case in ai_testcases) / current_count if current_count > 0 else 0
        steps_change = current_avg_steps - prev_avg_steps
        steps_change_percent = (steps_change / prev_avg_steps * 100) if prev_avg_steps > 0 else 0
        
        prev_avg_expected = sum(len(case.get("expected_results", [])) for case in prev_testcases) / prev_count if prev_count > 0 else 0
        current_avg_expected = sum(len(case.get("expected_results", [])) for case in ai_testcases) / current_count if current_count > 0 else 0
        expected_change = current_avg_expected - prev_avg_expected
        expected_change_percent = (expected_change / prev_avg_expected * 100) if prev_avg_expected > 0 else 0
        
        iteration_comparison_text += f"""
- å¹³å‡æ­¥éª¤æ•°å˜åŒ–: {prev_avg_steps:.2f} â†’ {current_avg_steps:.2f} ({steps_change_percent:.2f}%)
- å¹³å‡é¢„æœŸç»“æœæ•°å˜åŒ–: {prev_avg_expected:.2f} â†’ {current_avg_expected:.2f} ({expected_change_percent:.2f}%)

è¯·æ ¹æ®ä»¥ä¸Šå¯¹æ¯”ä¿¡æ¯ï¼Œåˆ†æå½“å‰è¿­ä»£ç›¸æ¯”ä¸Šä¸€æ¬¡è¿­ä»£çš„ä¸»è¦ä¼˜ç¼ºç‚¹ï¼Œå¹¶æå‡ºå…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚
"""
        
        log("å·²ç”Ÿæˆè¿­ä»£å‰åå¯¹æ¯”åˆ†æ", important=True)

    # å¦‚æœæœ‰åˆå¹¶å»ºè®®ï¼Œæ·»åŠ åˆ°æç¤ºä¸­
    if merge_suggestions_count > 0:
        duplicate_info_text += "\n## AIæµ‹è¯•ç”¨ä¾‹åˆå¹¶å»ºè®®\n"
        for i, suggestion in enumerate(ai_duplicate_info.get("merge_suggestions", [])):
            suggestion_type = "æ ‡é¢˜é‡å¤" if suggestion["type"] == "title_duplicate" else "æ­¥éª¤ç›¸ä¼¼"
            case_ids = ", ".join(suggestion["case_ids"][:3])
            if len(suggestion["case_ids"]) > 3:
                case_ids += f" ç­‰{len(suggestion['case_ids'])}ä¸ªç”¨ä¾‹"

            merged_case = suggestion["merged_case"]
            duplicate_info_text += f"\n### åˆå¹¶å»ºè®® {i + 1}ï¼ˆ{suggestion_type}ï¼‰\n"
            duplicate_info_text += f"- æ¶‰åŠç”¨ä¾‹: {case_ids}\n"
            duplicate_info_text += f"- åˆå¹¶åæ ‡é¢˜: {merged_case['title']}\n"

            # æ·»åŠ æ­¥éª¤å’Œé¢„æœŸç»“æœæ‘˜è¦
            steps = merged_case.get("steps", "")
            if isinstance(steps, list) and len(steps) > 0:
                steps_preview = steps[0]
                if len(steps) > 1:
                    steps_preview += f" ... ç­‰{len(steps)}ä¸ªæ­¥éª¤"
                duplicate_info_text += f"- åˆå¹¶åæ­¥éª¤: {steps_preview}\n"

            expected = merged_case.get("expected_results", "")
            if isinstance(expected, list) and len(expected) > 0:
                expected_preview = expected[0]
                if len(expected) > 1:
                    expected_preview += f" ... ç­‰{len(expected)}ä¸ªé¢„æœŸç»“æœ"
                duplicate_info_text += f"- åˆå¹¶åé¢„æœŸç»“æœ: {expected_preview}\n"

    # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨å¤šè¯„å§”å§”å‘˜ä¼šè¯„æµ‹
    if ENABLE_MULTI_JUDGES and COMMITTEE_IMPORTED:
        log("å¯ç”¨å¤šè¯„å§”å§”å‘˜ä¼šè¯„æµ‹", important=True)
        
        # å¦‚æœå¯ç”¨äº†CollabEvalæ¡†æ¶ä¸”ä¸æ˜¯è¿­ä»£å¯¹æ¯”æ¨¡å¼ï¼Œè®°å½•åˆ°æ—¥å¿—
        use_collab_eval = ENABLE_COLLAB_EVAL and not is_iteration
        if use_collab_eval:
            log("ä½¿ç”¨CollabEvalä¸‰é˜¶æ®µè¯„æµ‹æ¡†æ¶ (ç‹¬ç«‹è¯„åˆ†->è¾©è®ºåä½œ->ä¸»å¸­èšåˆ)", important=True)
        else:
            if is_iteration:
                log("è¿­ä»£å¯¹æ¯”æ¨¡å¼ä¸‹ä½¿ç”¨æ ‡å‡†å¤šè¯„å§”è¯„æµ‹æ¡†æ¶ï¼Œä¸å¯ç”¨CollabEval", important=True)
            else:
                log("ä½¿ç”¨æ ‡å‡†å¤šè¯„å§”è¯„æµ‹æ¡†æ¶ (ç‹¬ç«‹è¯„åˆ†->ç»“æœèšåˆ)", important=True)
            
        try:
            # è°ƒç”¨å§”å‘˜ä¼šè¯„æµ‹ï¼Œæ·»åŠ è¿­ä»£å¯¹æ¯”ä¿¡æ¯
            if is_iteration and iteration_comparison_text:
                evaluation_result = await evaluate_with_committee(
                    session,
                    ai_testcases,
                    golden_testcases,
                    duplicate_info_text + "\n" + iteration_comparison_text,
                    use_collab_eval=False  # è¿­ä»£å¯¹æ¯”æ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨æ ‡å‡†å¤šè¯„å§”è¯„æµ‹
                )
            else:
                evaluation_result = await evaluate_with_committee(
                    session,
                    ai_testcases,
                    golden_testcases,
                    duplicate_info_text,
                    use_collab_eval=use_collab_eval  # æ ¹æ®æ¡ä»¶å†³å®šæ˜¯å¦ä½¿ç”¨CollabEval
                )

            if evaluation_result:
                if use_collab_eval:
                    log("CollabEvalä¸‰é˜¶æ®µè¯„æµ‹å®Œæˆ", important=True)
                else:
                    log("å¤šè¯„å§”å§”å‘˜ä¼šè¯„æµ‹å®Œæˆ", important=True)

                # å°†é‡å¤æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯æ·»åŠ åˆ°è¯„æµ‹ç»“æœä¸­
                evaluation_result["duplicate_types"] = ai_duplicate_info['duplicate_types']
                evaluation_result["duplicate_categories"] = ai_duplicate_info.get('duplicate_categories', {})
                evaluation_result["duplicate_info"] = {
                    "ai_duplicate_rate": ai_duplicate_info['duplicate_rate'],
                    "golden_duplicate_rate": golden_duplicate_info['duplicate_rate'],
                    "merge_suggestions": ai_duplicate_info.get("merge_suggestions", [])
                }
                
                # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œæ·»åŠ è¿­ä»£å¯¹æ¯”ä¿¡æ¯
                if is_iteration and prev_testcases:
                    evaluation_result["iteration_comparison"] = {
                        "is_iteration": True,
                        "prev_count": len(prev_testcases),
                        "current_count": len(ai_testcases),
                        "count_change": len(ai_testcases) - len(prev_testcases),
                        "prev_duplicate_rate": prev_duplicate_info['duplicate_rate'],
                        "current_duplicate_rate": ai_duplicate_info['duplicate_rate'],
                        "new_categories": list(new_categories) if 'new_categories' in locals() else [],
                        "removed_categories": list(removed_categories) if 'removed_categories' in locals() else []
                    }

                # æ˜ç¡®æ ‡è®°ä¸ºç»¼åˆè¯„æµ‹ç»“æœ
                if "evaluation_summary" in evaluation_result:
                    if "final_suggestion" in evaluation_result["evaluation_summary"]:
                        # æ£€æŸ¥è¯„ä¼°æ¡†æ¶
                        framework_type = "Standard"
                        if "evaluation_framework" in evaluation_result:
                            framework_type = evaluation_result["evaluation_framework"]
                        elif "committee_summary" in evaluation_result and "evaluation_framework" in evaluation_result["committee_summary"]:
                            framework_type = evaluation_result["committee_summary"]["evaluation_framework"]
                        
                        # ç¡®å®šå‰ç¼€
                        prefix = "ã€CollabEvalä¸‰é˜¶æ®µç»¼åˆè¯„æµ‹ã€‘" if framework_type == "CollabEval" else "ã€å¤šè¯„å§”ç»¼åˆè¯„æµ‹ã€‘"
                        if is_iteration:
                            prefix = "ã€è¿­ä»£å¯¹æ¯”åˆ†æã€‘" + prefix
                        evaluation_result["evaluation_summary"]["final_suggestion"] = prefix + \
                                                                                     evaluation_result[
                                                                                         "evaluation_summary"][
                                                                                         "final_suggestion"]

                    # æ·»åŠ ç»¼åˆè¯„æµ‹æ ‡è®°
                    evaluation_result["is_committee_result"] = True
                    evaluation_result["collab_eval_result"] = framework_type == "CollabEval"
                    evaluation_result["evaluation_framework"] = framework_type
                    evaluation_result["is_iteration_comparison"] = is_iteration and bool(prev_testcases)
                    evaluation_result["committee_info"] = {
                        "judge_count": len(evaluation_result.get("committee_summary", {}).get("judge_scores", {})),
                        "judges": list(evaluation_result.get("committee_summary", {}).get("judge_scores", {}).keys()),
                        "evaluation_framework": framework_type
                    }

                return evaluation_result
            else:
                log_error("å¤šè¯„å§”å§”å‘˜ä¼šè¯„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°å•ä¸€æ¨¡å‹è¯„æµ‹", important=True)
                # å¦‚æœå§”å‘˜ä¼šè¯„æµ‹å¤±è´¥ï¼Œå›é€€åˆ°å•ä¸€æ¨¡å‹è¯„æµ‹
        except Exception as e:
            log_error(f"å¤šè¯„å§”å§”å‘˜ä¼šè¯„æµ‹å‡ºé”™: {e}", important=True)
            log("å›é€€åˆ°å•ä¸€æ¨¡å‹è¯„æµ‹", important=True)

    # å•ä¸€æ¨¡å‹è¯„æµ‹æµç¨‹
    log("ä½¿ç”¨å•ä¸€æ¨¡å‹è¿›è¡Œè¯„æµ‹", important=True)

    # æ„å»ºå®Œæ•´æç¤º
    prompt = f"""
# ä»»åŠ¡
è¯„ä¼°AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹ä¸é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹çš„è´¨é‡å¯¹æ¯”ã€‚
"""

    # å¦‚æœæ˜¯è¿­ä»£å¯¹æ¯”ï¼Œå¢åŠ è¿­ä»£å¯¹æ¯”ä»»åŠ¡è¯´æ˜
    if is_iteration and prev_testcases:
        prompt += f"""
# è¿­ä»£å¯¹æ¯”ä»»åŠ¡
æœ¬æ¬¡è¯„ä¼°åŒ…å«è¿­ä»£å‰åå¯¹æ¯”åˆ†æï¼Œéœ€è¦é‡ç‚¹å…³æ³¨æµ‹è¯•ç”¨ä¾‹åœ¨æœ¬æ¬¡è¿­ä»£ä¸­çš„è´¨é‡æ”¹è¿›æƒ…å†µï¼Œå¹¶æå‡ºé’ˆå¯¹æ€§å»ºè®®ã€‚
"""

    prompt += """
# è¯„ä¼°ç»´åº¦å’Œæƒé‡
1. **åŠŸèƒ½è¦†ç›–åº¦**ï¼ˆæƒé‡30%ï¼‰ï¼šè¯„ä¼°éœ€æ±‚è¦†ç›–ç‡ã€è¾¹ç•Œå€¼è¦†ç›–åº¦ã€åˆ†æ”¯è·¯å¾„è¦†ç›–ç‡
2. **ç¼ºé™·å‘ç°èƒ½åŠ›**ï¼ˆæƒé‡25%ï¼‰ï¼šè¯„ä¼°ç¼ºé™·æ£€æµ‹ç‡ã€çªå˜åˆ†æ•°ã€å¤±è´¥ç”¨ä¾‹æ¯”ä¾‹
3. **å·¥ç¨‹æ•ˆç‡**ï¼ˆæƒé‡20%ï¼‰ï¼šè¯„ä¼°æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆé€Ÿåº¦ã€ç»´æŠ¤æˆæœ¬ã€CI/CDé›†æˆåº¦
4. **è¯­ä¹‰è´¨é‡**ï¼ˆæƒé‡15%ï¼‰ï¼šè¯„ä¼°è¯­ä¹‰å‡†ç¡®æ€§ã€äººå·¥å¯è¯»æ€§ã€æ–­è¨€æè¿°æ¸…æ™°åº¦
5. **å®‰å…¨ä¸ç»æµæ€§**ï¼ˆæƒé‡10%ï¼‰ï¼šè¯„ä¼°æ¶æ„ä»£ç ç‡ã€å†—ä½™ç”¨ä¾‹æ¯”ä¾‹ã€ç»¼åˆæˆæœ¬
"""

    # æ·»åŠ é‡å¤æµ‹è¯•ç”¨ä¾‹ä¿¡æ¯
    prompt += "\n" + duplicate_info_text + "\n"
    
    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œæ·»åŠ è¿­ä»£å¯¹æ¯”ä¿¡æ¯
    if is_iteration and iteration_comparison_text:
        prompt += "\n" + iteration_comparison_text + "\n"

    # æ·»åŠ è¯„åˆ†å…¬å¼
    prompt += """
# è¯„åˆ†å…¬å¼
æ€»åˆ† = 0.3Ã—åŠŸèƒ½è¦†ç›–å¾—åˆ† + 0.25Ã—ç¼ºé™·å‘ç°å¾—åˆ† + 0.2Ã—å·¥ç¨‹æ•ˆç‡å¾—åˆ† + 0.15Ã—è¯­ä¹‰è´¨é‡å¾—åˆ† + 0.1Ã—å®‰å…¨ç»æµå¾—åˆ†
å„ç»´åº¦å¾—åˆ† = (AIæŒ‡æ ‡å€¼/äººå·¥åŸºå‡†å€¼)Ã—10ï¼ˆæ»¡åˆ†10åˆ†ï¼‰

# AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
```json
"""
    # æ·»åŠ AIæµ‹è¯•ç”¨ä¾‹
    prompt += json.dumps(ai_testcases, ensure_ascii=False, indent=2) + "\n```\n\n"

    # æ·»åŠ é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
    prompt += """
# é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
```json
"""
    prompt += json.dumps(golden_testcases, ensure_ascii=False, indent=2) + "\n```\n\n"
    
    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œæ·»åŠ ä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹
    if is_iteration and prev_testcases:
        prompt += """
# ä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹
```json
"""
        prompt += json.dumps(prev_testcases, ensure_ascii=False, indent=2) + "\n```\n\n"

    # æ·»åŠ è¾“å‡ºè¦æ±‚
    prompt += """
# è¾“å‡ºè¦æ±‚
å¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºè¯„ä¼°ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–å†…å®¹ï¼Œä¸è¦ä½¿ç”¨```jsonæˆ–å…¶ä»–ä»£ç å—åŒ…è£…ï¼Œä¸è¦è¿”å›Markdownæ ¼å¼å†…å®¹ã€‚ç›´æ¥è¾“å‡ºä¸‹é¢è¿™ç§JSONç»“æ„ï¼š

```json
{
  "evaluation_summary": {
    "overall_score": "åˆ†æ•°ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
    "final_suggestion": "å¦‚ä½•æ”¹è¿›æµ‹è¯•ç”¨ä¾‹ç”Ÿæˆçš„å»ºè®®ï¼Œå¦‚æœ‰è¾ƒé«˜çš„é‡å¤ç‡ï¼Œè¯·æå‡ºé™ä½é‡å¤çš„å»ºè®®"
  },
  "detailed_report": {
    "format_compliance": {
      "score": "æ ¼å¼åˆè§„æ€§å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±"
    },
    "content_accuracy": {
      "score": "å†…å®¹å‡†ç¡®æ€§å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±"
    },
    "test_coverage": {
      "score": "æµ‹è¯•è¦†ç›–åº¦å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±",
      "analysis": {
        "covered_features": [
          "å·²è¦†ç›–åŠŸèƒ½1",
          "å·²è¦†ç›–åŠŸèƒ½2"
        ],
        "missed_features_or_scenarios": [
          "æœªè¦†ç›–åŠŸèƒ½/åœºæ™¯1",
          "æœªè¦†ç›–åŠŸèƒ½/åœºæ™¯2"
        ],
        "scenario_types_found": [
          "å‘ç°çš„åœºæ™¯ç±»å‹ï¼Œå¦‚æ­£é¢ç”¨ä¾‹ã€è´Ÿé¢ç”¨ä¾‹ã€è¾¹ç•Œç”¨ä¾‹ç­‰"
        ]
      }
    },
    "functional_coverage": {
      "score": "åŠŸèƒ½è¦†ç›–åº¦å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±"
    },
    "defect_detection": {
      "score": "ç¼ºé™·å‘ç°èƒ½åŠ›å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±"
    },
    "engineering_efficiency": {
      "score": "å·¥ç¨‹æ•ˆç‡å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±ï¼Œå¦‚æœ‰è¾ƒé«˜çš„é‡å¤ç‡ï¼Œè¯·åœ¨æ­¤å¤„æåŠ"
    },
    "semantic_quality": {
      "score": "è¯­ä¹‰è´¨é‡å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±"
    },
    "security_economy": {
      "score": "å®‰å…¨ä¸ç»æµæ€§å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¾—åˆ†ç†ç”±ï¼Œå¦‚æœ‰è¾ƒé«˜çš„é‡å¤ç‡ï¼Œè¯·åœ¨æ­¤å¤„æåŠå†—ä½™ç‡"
    },
    "duplicate_analysis": {
      "score": "æµ‹è¯•ç”¨ä¾‹é‡å¤åˆ†æå¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "åˆ†æé‡å¤æµ‹è¯•ç”¨ä¾‹çš„å½±å“",
      "merge_suggestions": "å…·ä½“å¦‚ä½•åˆå¹¶é‡å¤æµ‹è¯•ç”¨ä¾‹çš„å»ºè®®ï¼Œå¯ä»¥å‚è€ƒæˆ‘æä¾›çš„åˆå¹¶å»ºè®®"
    }"""
    
    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œæ·»åŠ è¿­ä»£å¯¹æ¯”è¯„ä¼°ç»´åº¦
    if is_iteration:
        prompt += """,
    "iteration_comparison": {
      "score": "è¿­ä»£æ”¹è¿›å¾—åˆ†ï¼ˆ1-5ä¹‹é—´çš„ä¸€ä½å°æ•°ï¼‰",
      "reason": "å¯¹æ¯”æœ¬æ¬¡è¿­ä»£ä¸ä¸Šä¸€æ¬¡è¿­ä»£çš„æ”¹è¿›æƒ…å†µï¼Œåˆ†æä¸»è¦ä¼˜åŠ¿å’Œä¸è¶³",
      "key_improvements": [
        "ä¸»è¦æ”¹è¿›ç‚¹1",
        "ä¸»è¦æ”¹è¿›ç‚¹2"
      ],
      "key_regressions": [
        "ä¸»è¦é€€æ­¥ç‚¹1",
        "ä¸»è¦é€€æ­¥ç‚¹2"
      ],
      "next_iteration_suggestions": [
        "ä¸‹ä¸€æ¬¡è¿­ä»£æ”¹è¿›å»ºè®®1",
        "ä¸‹ä¸€æ¬¡è¿­ä»£æ”¹è¿›å»ºè®®2"
      ]
    }"""
    
    prompt += """
  }"""
    
    # æ·»åŠ é‡å¤ç±»å‹ä¿¡æ¯
    prompt += f""",
  "duplicate_types": {{
    "title": {ai_duplicate_info['duplicate_types'].get('title', 0)},
    "steps": {ai_duplicate_info['duplicate_types'].get('steps', 0)},
    "expected_results": {ai_duplicate_info['duplicate_types'].get('expected_results', 0)},
    "mixed": {ai_duplicate_info['duplicate_types'].get('mixed', 0)}
  }},
  "duplicate_categories": {json.dumps(ai_duplicate_info.get('duplicate_categories', {}))}"""
    
    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œæ·»åŠ è¿­ä»£å¯¹æ¯”ç»“æœ
    if is_iteration and prev_testcases:
        prompt += f""",
  "iteration_comparison_data": {{
    "prev_count": {len(prev_testcases)},
    "current_count": {len(ai_testcases)},
    "count_change_percent": {(len(ai_testcases) - len(prev_testcases)) / len(prev_testcases) * 100 if len(prev_testcases) > 0 else 0:.2f},
    "prev_duplicate_rate": {prev_duplicate_info['duplicate_rate']},
    "current_duplicate_rate": {ai_duplicate_info['duplicate_rate']},
    "duplicate_rate_change": {ai_duplicate_info['duplicate_rate'] - prev_duplicate_info['duplicate_rate']:.2f},
    "new_categories_count": {len(new_categories) if 'new_categories' in locals() else 0},
    "removed_categories_count": {len(removed_categories) if 'removed_categories' in locals() else 0}
  }}"""
    
    prompt += """
}
```
"""

    system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šè½¯ä»¶æµ‹è¯•å’ŒæŠ€æœ¯æ–‡æ¡£å†™ä½œçš„ä¸“å®¶ã€‚è¯·æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸…æ™°çš„Markdownæ ¼å¼æŠ¥å‘Šï¼Œå¹¶ä½¿ç”¨Mermaidå›¾è¡¨å¯è§†åŒ–å…³é”®æ•°æ®ã€‚è¯·ç›´æ¥ä¿ç•™å¹¶ä½¿ç”¨æˆ‘æä¾›çš„è¯„åˆ†è¡¨æ ¼æ ¼å¼ï¼Œä¸è¦ä¿®æ”¹å…¶ç»“æ„ã€‚è¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼ï¼Œä¸è¦å°è¯•è¾“å‡ºJSONã€‚ä¸¥æ ¼ç¦æ­¢åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ 'markdown'è¿™ä¸ªè¯ï¼Œç›´æ¥ä»¥'# 'å¼€å¤´çš„æ ‡é¢˜å¼€å§‹ã€‚ä¸è¦åœ¨å†…å®¹å¤–åŒ…å«```æˆ–```markdownæ ‡è®°ï¼Œå®Œå…¨é¿å…ä½¿ç”¨ä»£ç å—ï¼Œä½†ä¿ç•™æä¾›çš„Mermaidå›¾è¡¨è¯­æ³•ã€‚"

    # ä½¿ç”¨è¾ƒä½çš„temperatureå€¼ï¼Œç¡®ä¿è¯„æµ‹ç»“æœçš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§
    result = await async_call_llm(
        session,
        prompt,
        system_prompt,
        temperature=LLM_TEMPERATURE,  # ä½¿ç”¨é…ç½®ä¸­çš„ä½temperatureå€¼
        use_cache=False  # ç¦ç”¨ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡è¯„æµ‹éƒ½æ˜¯å…¨æ–°çš„
    )

    if not result:
        log("æµ‹è¯•ç”¨ä¾‹è¯„æµ‹å¤±è´¥", important=True)
        return None

    log("æµ‹è¯•ç”¨ä¾‹è¯„æµ‹å®Œæˆ", important=True)
    return result


# æ·»åŠ æµ‹è¯•è¦†ç›–æµç¨‹å›¾ç”Ÿæˆå‡½æ•°
def generate_test_coverage_flow_chart(test_cases, evaluation_result=None):
    """
    æ ¹æ®æµ‹è¯•ç”¨ä¾‹å†…å®¹åŠ¨æ€ç”Ÿæˆæµ‹è¯•è¦†ç›–æµç¨‹å›¾

    :param test_cases: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨æˆ–åŒ…å«åˆ†ç±»æµ‹è¯•ç”¨ä¾‹çš„å­—å…¸
    :param evaluation_result: è¯„æµ‹ç»“æœæ•°æ®ï¼ˆå¯é€‰ï¼‰
    :return: Mermaidæ ¼å¼çš„æµç¨‹å›¾
    """
    # ä»é…ç½®æ–‡ä»¶å¯¼å…¥æµ‹è¯•è¦†ç›–ç‡åˆ†æç›¸å…³é…ç½®
    from config import COVERAGE_KEYWORDS, COVERAGE_FULL_THRESHOLD, COVERAGE_PARTIAL_THRESHOLD
    
    # è¾…åŠ©å‡½æ•°ï¼šè½¬ä¹‰Mermaidå›¾è¡¨ä¸­çš„ç‰¹æ®Šå­—ç¬¦
    def escape_mermaid_text(text):
        """è½¬ä¹‰Mermaidå›¾è¡¨ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼Œç¡®ä¿èŠ‚ç‚¹æ–‡æœ¬æ­£ç¡®æ˜¾ç¤º"""
        if not isinstance(text, str):
            return str(text)
        
        # æ›¿æ¢åŒå¼•å·ä¸ºå•å¼•å·ï¼Œé¿å…Mermaidè¯­æ³•é”™è¯¯
        text = text.replace('"', "'")
        # æ›¿æ¢ä¸­æ–‡åŒå¼•å·ä¸ºå•å¼•å·
        text = text.replace("â€œ", "'")
        text = text.replace("â€", "'")
        # å¤„ç†å…¨è§’ç›´å¼•å·
        text = text.replace('ï¼‚', "'")
        # å¤„ç†ä¸­æ–‡ç›´è§’å¼•å·
        text = text.replace('ã€', "'")
        text = text.replace('ã€', "'")
        
        # æ›¿æ¢å…¶ä»–å¯èƒ½å¯¼è‡´Mermaidè¯­æ³•é”™è¯¯çš„å­—ç¬¦
        text = text.replace("[", "(")
        text = text.replace("]", ")")
        text = text.replace("{", "(")
        text = text.replace("}", ")")
        
        return text
    
    # åˆå§‹åŒ–è¦†ç›–çŠ¶æ€
    coverage_status = {
        "åŠŸèƒ½éªŒè¯": "missing",  # é»˜è®¤ä¸ºæœªè¦†ç›–
        "å¼‚å¸¸å¤„ç†": "missing",
        "è¾¹ç•Œæµ‹è¯•": "missing",
        "è¾“å…¥éªŒè¯": "missing",
        "è¶…æ—¶å¤„ç†": "missing",
        "å®‰å…¨æ£€æŸ¥": "missing",
        "æœ€å¤§å€¼æµ‹è¯•": "missing",
        "æœ€å°å€¼æµ‹è¯•": "missing"
    }
    
    # åŠŸèƒ½åˆ†ç±»è®¡æ•°å™¨
    feature_counts = {
        "åŠŸèƒ½éªŒè¯": 0,
        "å¼‚å¸¸å¤„ç†": 0,
        "è¾¹ç•Œæµ‹è¯•": 0,
        "è¾“å…¥éªŒè¯": 0,
        "è¶…æ—¶å¤„ç†": 0,
        "å®‰å…¨æ£€æŸ¥": 0,
        "æœ€å¤§å€¼æµ‹è¯•": 0,
        "æœ€å°å€¼æµ‹è¯•": 0
    }
    
    # æå–æµ‹è¯•ç”¨ä¾‹IDä¸­çš„åŠŸèƒ½æ¨¡å—ä¿¡æ¯
    modules = {}
    submodules = {}

    # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„å…³é”®è¯åˆ—è¡¨
    keywords = COVERAGE_KEYWORDS
    
    # å¤„ç†æµ‹è¯•ç”¨ä¾‹ï¼Œç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
    all_test_cases = []
    
    # æ£€æŸ¥test_casesæ˜¯å¦æ˜¯å­—å…¸ï¼ˆåŒ…å«åˆ†ç±»çš„æµ‹è¯•ç”¨ä¾‹ï¼‰
    if isinstance(test_cases, dict):
        # éå†å­—å…¸ä¸­çš„æ¯ä¸ªåˆ†ç±»
        for category, cases in test_cases.items():
            if isinstance(cases, list):
                # ä¸ºæ¯ä¸ªæµ‹è¯•ç”¨ä¾‹æ·»åŠ categoryæ ‡è®°
                for case in cases:
                    if isinstance(case, dict):
                        case_copy = case.copy()
                        case_copy["category"] = category.lower()
                        all_test_cases.append(case_copy)
    elif isinstance(test_cases, list):
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
        all_test_cases = test_cases
    
    # å¦‚æœall_test_casesä¸ºç©ºï¼Œå°è¯•å…¶ä»–æ ¼å¼è§£æ
    if not all_test_cases:
        # å°è¯•å¤„ç†testcases.test_casesæ ¼å¼
        if isinstance(test_cases, dict) and "testcases" in test_cases:
            if isinstance(test_cases["testcases"], dict) and "test_cases" in test_cases["testcases"]:
                test_cases_data = test_cases["testcases"]["test_cases"]
                if isinstance(test_cases_data, dict):
                    # å¤„ç†æŒ‰ç±»åˆ«ç»„ç»‡çš„æµ‹è¯•ç”¨ä¾‹
                    for category, cases in test_cases_data.items():
                        if isinstance(cases, list):
                            for case in cases:
                                if isinstance(case, dict):
                                    case_copy = case.copy()
                                    case_copy["category"] = category.lower()
                                    all_test_cases.append(case_copy)
                elif isinstance(test_cases_data, list):
                    all_test_cases = test_cases_data

    # ä»æµ‹è¯•ç”¨ä¾‹åˆ†æè¦†ç›–æƒ…å†µ
    for case in all_test_cases:
        case_id = case.get("case_id", "")
        title = case.get("title", "").lower()
        category = case.get("category", "").lower()
        
        # ç›´æ¥æ£€æŸ¥categoryæ˜¯å¦ä¸ºç‰¹å®šç±»å‹
        if category == "functional" or case_id.startswith("FT-"):
            feature_counts["åŠŸèƒ½éªŒè¯"] += 1
        
        if category == "security" or case_id.startswith("ST-"):
            feature_counts["å®‰å…¨æ£€æŸ¥"] += 1
        
        if category == "exception" or case_id.startswith("ET-"):
            feature_counts["å¼‚å¸¸å¤„ç†"] += 1
            
        if category == "boundary" or case_id.startswith("BT-"):
            feature_counts["è¾¹ç•Œæµ‹è¯•"] += 1
            # å¯¹äºè¾¹ç•Œæµ‹è¯•ï¼Œé»˜è®¤ä¹Ÿå¢åŠ æœ€å¤§å€¼å’Œæœ€å°å€¼æµ‹è¯•çš„è®¡æ•°
            # é™¤éæ ‡é¢˜ä¸­æ˜ç¡®æŒ‡å‡ºæ˜¯æœ€å¤§å€¼æˆ–æœ€å°å€¼æµ‹è¯•
            if "æœ€å¤§" in title or "ä¸Šé™" in title or "æœ€é«˜" in title or "æœ€å¤š" in title:
                feature_counts["æœ€å¤§å€¼æµ‹è¯•"] += 1
            elif "æœ€å°" in title or "ä¸‹é™" in title or "æœ€ä½" in title or "æœ€å°‘" in title:
                feature_counts["æœ€å°å€¼æµ‹è¯•"] += 1
            else:
                # å¦‚æœæ²¡æœ‰æ˜ç¡®æŒ‡å‡ºï¼Œåˆ™åŒæ—¶å¢åŠ ä¸¤è€…çš„è®¡æ•°
                feature_counts["æœ€å¤§å€¼æµ‹è¯•"] += 1
                feature_counts["æœ€å°å€¼æµ‹è¯•"] += 1
        
        # æ·±å…¥åˆ†ææµ‹è¯•ç”¨ä¾‹å†…å®¹
        steps = []
        if "steps" in case and isinstance(case["steps"], list):
            steps = [step.lower() if isinstance(step, str) else "" for step in case["steps"]]
        
        expected_results = []
        if "expected_results" in case and isinstance(case["expected_results"], list):
            expected_results = [result.lower() if isinstance(result, str) else "" for result in case["expected_results"]]
        elif "expected_results" in case and isinstance(case["expected_results"], str):
            expected_results = [case["expected_results"].lower()]
        
        preconditions = ""
        if "preconditions" in case and case["preconditions"]:
            if isinstance(case["preconditions"], str):
                preconditions = case["preconditions"].lower()
            elif isinstance(case["preconditions"], list):
                preconditions = " ".join([p.lower() if isinstance(p, str) else "" for p in case["preconditions"]])
        
        # åˆå¹¶æ‰€æœ‰æ–‡æœ¬å†…å®¹è¿›è¡Œåˆ†æ
        all_text = title + " " + category + " " + case_id + " " + preconditions + " " + " ".join(steps) + " " + " ".join(expected_results)
        
        # åŸºäºå…³é”®è¯åˆ†æè¦†ç›–ç±»å‹
        for feature, feature_keywords in keywords.items():
            for keyword in feature_keywords:
                if keyword.lower() in all_text:
                    feature_counts[feature] += 1
                    break  # æ‰¾åˆ°ä¸€ä¸ªå…³é”®è¯å°±è·³å‡ºå†…å±‚å¾ªç¯
        
        # ç‰¹æ®Šè§„åˆ™ï¼šå¦‚æœæµ‹è¯•ç”¨ä¾‹åŒ…å«"ç™»å½•"ã€"æ³¨å†Œ"ç­‰æ ¸å¿ƒåŠŸèƒ½è¯ï¼Œè§†ä¸ºåŠŸèƒ½éªŒè¯
        core_features = ["ç™»å½•", "æ³¨å†Œ", "æŸ¥è¯¢", "æœç´¢", "åˆ›å»º", "åˆ é™¤", "ä¿®æ”¹", "æ›´æ–°", "ä¸Šä¼ ", "ä¸‹è½½"]
        for core in core_features:
            if core in all_text:
                feature_counts["åŠŸèƒ½éªŒè¯"] += 1
                break
        
        # ç‰¹æ®Šè§„åˆ™ï¼šå¦‚æœæµ‹è¯•ç”¨ä¾‹æè¿°äº†è¾“å…¥éªŒè¯ç›¸å…³å†…å®¹
        input_validation_patterns = ["è¾“å…¥", "å¡«å†™", "è¾“å…¥æ¡†", "å­—æ®µ", "è¡¨å•", "å¿…å¡«", "é€‰å¡«", "æœ‰æ•ˆ", "æ— æ•ˆ"]
        for pattern in input_validation_patterns:
            if pattern in all_text and ("éªŒè¯" in all_text or "æ£€æŸ¥" in all_text or "æ ¡éªŒ" in all_text):
                feature_counts["è¾“å…¥éªŒè¯"] += 1
                break
        
        # ç‰¹æ®Šè§„åˆ™ï¼šè¯†åˆ«è¾¹ç•Œæµ‹è¯• - åŸºäºå†…å®¹å…³é”®è¯
        if not (category == "boundary" or case_id.startswith("BT-")):  # é¿å…é‡å¤è®¡æ•°
            if any(boundary in all_text for boundary in ["è¾¹ç•Œ", "æé™", "ä¸´ç•Œ"]):
                if any(value in all_text for value in ["å€¼", "æ•°é‡", "é•¿åº¦", "å¤§å°", "èŒƒå›´"]):
                    feature_counts["è¾¹ç•Œæµ‹è¯•"] += 1
                    # ç»†åˆ†ä¸ºæœ€å¤§å€¼æˆ–æœ€å°å€¼æµ‹è¯•
                    if any(max_val in all_text for max_val in ["æœ€å¤§", "ä¸Šé™", "æœ€é«˜", "æœ€å¤š"]):
                        feature_counts["æœ€å¤§å€¼æµ‹è¯•"] += 1
                    if any(min_val in all_text for min_val in ["æœ€å°", "ä¸‹é™", "æœ€ä½", "æœ€å°‘"]):
                        feature_counts["æœ€å°å€¼æµ‹è¯•"] += 1

        # æå–ä¸»è¦åŠŸèƒ½æ¨¡å—å’Œå­åŠŸèƒ½æ¨¡å—
        parts = case_id.split('-')
        if len(parts) >= 2:
            main_module = parts[0]
            if len(parts) >= 3:
                sub_module = parts[1]

                # è®°å½•æ¨¡å—
                if main_module not in modules:
                    modules[main_module] = 0
                modules[main_module] += 1

                # è®°å½•å­æ¨¡å—
                module_key = f"{main_module}-{sub_module}"
                if module_key not in submodules:
                    submodules[module_key] = {
                        "name": sub_module,
                        "parent": main_module,
                        "count": 0
                    }
                submodules[module_key]["count"] += 1

    # æ ¹æ®åŠŸèƒ½è®¡æ•°è°ƒæ•´è¦†ç›–çŠ¶æ€
    # å¦‚æœè®¡æ•°å¤§äº0ä½†è¯„ä¼°ç»“æœæœªè¦†ç›–ï¼Œè®¾ä¸º"partial"
    for feature, count in feature_counts.items():
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é˜ˆå€¼ï¼Œä½†ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç”¨ä¾‹å°±æ ‡è®°ä¸ºcovered
        if count >= COVERAGE_FULL_THRESHOLD:  # å¦‚æœæœ‰è¶³å¤Ÿå¤šçš„ç”¨ä¾‹è¦†ç›–è¯¥åŠŸèƒ½ï¼Œè®¤ä¸ºæ˜¯å®Œå…¨è¦†ç›–
            coverage_status[feature] = "covered"
        elif count >= COVERAGE_PARTIAL_THRESHOLD:  # å¦‚æœæœ‰è‡³å°‘ä¸€å®šæ•°é‡çš„ç”¨ä¾‹è¦†ç›–è¯¥åŠŸèƒ½ï¼Œè®¤ä¸ºæ˜¯éƒ¨åˆ†è¦†ç›–
            coverage_status[feature] = "partial"
        
        # ç‰¹åˆ«å¤„ç†ï¼šç¡®ä¿æœ‰ä»»ä½•è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹æ—¶ï¼Œè¾¹ç•Œæµ‹è¯•çŠ¶æ€ä¸ºcovered
        if feature == "è¾¹ç•Œæµ‹è¯•" and count > 0:
            coverage_status[feature] = "covered"

    # è¾¹ç•Œæµ‹è¯•ç‰¹æ®Šå¤„ç†ï¼šç¡®ä¿è¾¹ç•Œæµ‹è¯•ç”¨ä¾‹å­˜åœ¨æ—¶ï¼Œæœ€å¤§å€¼å’Œæœ€å°å€¼æµ‹è¯•çŠ¶æ€ä¹Ÿè¢«è®¾ç½®
    if feature_counts["è¾¹ç•Œæµ‹è¯•"] > 0:
        # ç¡®ä¿è¾¹ç•Œæµ‹è¯•æœ¬èº«è¢«æ ‡è®°ä¸ºè¦†ç›–
        coverage_status["è¾¹ç•Œæµ‹è¯•"] = "covered"
        
        # å¦‚æœæœ‰è¾¹ç•Œæµ‹è¯•ï¼Œä½†æ²¡æœ‰æ˜ç¡®çš„æœ€å¤§å€¼æˆ–æœ€å°å€¼æµ‹è¯•ï¼Œåˆ™å°†ä¸¤è€…è®¾ä¸ºéƒ¨åˆ†è¦†ç›–
        if feature_counts["æœ€å¤§å€¼æµ‹è¯•"] == 0:
            coverage_status["æœ€å¤§å€¼æµ‹è¯•"] = "partial"
        else:
            coverage_status["æœ€å¤§å€¼æµ‹è¯•"] = "covered"
        
        if feature_counts["æœ€å°å€¼æµ‹è¯•"] == 0:
            coverage_status["æœ€å°å€¼æµ‹è¯•"] = "partial"
        else:
            coverage_status["æœ€å°å€¼æµ‹è¯•"] = "covered"

    # æå–ä¸»è¦åŠŸèƒ½å’Œå­åŠŸèƒ½çš„å…³ç³»
    # å¦‚æœæµ‹è¯•ç”¨ä¾‹æ ‡é¢˜ä¸­åŒ…å«ç±»ä¼¼"xxæµç¨‹"ã€"xxåŠŸèƒ½"ã€"xxéªŒè¯"ç­‰è¯è¯­ï¼Œæå–ä¸ºåŠŸèƒ½ç‚¹
    features = {}
    for case in all_test_cases:
        title = case.get("title", "")
        if not title:
            continue

        # å°è¯•æå–åŠŸèƒ½ç‚¹
        feature = None
        if "æµç¨‹" in title:
            feature = title.split("æµç¨‹")[0] + "æµç¨‹"
        elif "åŠŸèƒ½" in title:
            feature = title.split("åŠŸèƒ½")[0] + "åŠŸèƒ½"
        elif "éªŒè¯" in title:
            feature = title.split("éªŒè¯")[0] + "éªŒè¯"
        elif "-" in title:
            feature = title.split("-")[0]
        elif "ï¼š" in title or ":" in title:
            feature = title.split("ï¼š")[0].split(":")[0]
        else:
            # å¦‚æœæ²¡æœ‰ç‰¹å®šæ ‡è¯†ï¼Œä½¿ç”¨å‰ä¸‰ä¸ªè¯ä½œä¸ºåŠŸèƒ½ç‚¹
            words = title.split()
            if words:
                feature = words[0]
                if len(words) > 1:
                    feature += words[1]

        if feature and len(feature) <= 20:  # é™åˆ¶åŠŸèƒ½ç‚¹åç§°é•¿åº¦
            if feature not in features:
                features[feature] = {
                    "count": 0,
                    "subfeatures": set()
                }
            features[feature]["count"] += 1

            # å°è¯•æå–å­åŠŸèƒ½ç‚¹
            steps = case.get("steps", [])
            for step in steps:
                if isinstance(step, str) and step:
                    # æå–æ­¥éª¤ä¸­çš„å…³é”®åŠ¨ä½œ
                    words = step.split("ã€‚")[0].split()
                    action = ""
                    for word in words:
                        if "ç‚¹å‡»" in word or "è¾“å…¥" in word or "é€‰æ‹©" in word or "éªŒè¯" in word:
                            action = word
                            break
                    
                    if action and len(action) <= 15:
                        features[feature]["subfeatures"].add(action)

    # æŒ‰æµ‹è¯•ç”¨ä¾‹æ•°é‡æ’åºåŠŸèƒ½ç‚¹
    sorted_features = sorted(features.items(), key=lambda x: x[1]["count"], reverse=True)

    # ç”ŸæˆMermaidå›¾è¡¨
    chart = "```mermaid\ngraph TD\n"

    # æ·»åŠ ä¸»èŠ‚ç‚¹ - ä½¿ç”¨è½¬ä¹‰å‡½æ•°å¤„ç†èŠ‚ç‚¹æ–‡æœ¬
    chart += f"    A[\"{escape_mermaid_text('æµ‹è¯•è¦†ç›–èŒƒå›´')}\"] --> B[\"{escape_mermaid_text('åŠŸèƒ½éªŒè¯')}\"]\n"
    chart += f"    A --> C[\"{escape_mermaid_text('å¼‚å¸¸å¤„ç†')}\"]\n"
    chart += f"    A --> D[\"{escape_mermaid_text('è¾¹ç•Œæµ‹è¯•')}\"]\n"

    # æ ¹æ®å®é™…åŠŸèƒ½ç‚¹åŠ¨æ€ç”Ÿæˆå­èŠ‚ç‚¹
    # å¦‚æœæœ‰æå–åˆ°å®é™…åŠŸèƒ½ç‚¹ï¼Œä½¿ç”¨å®ƒä»¬ï¼›å¦åˆ™ï¼Œä½¿ç”¨é»˜è®¤èŠ‚ç‚¹
    if sorted_features:
        # æ·»åŠ ä¸»è¦åŠŸèƒ½ç‚¹ï¼ˆæœ€å¤š6ä¸ªï¼Œé¿å…å›¾è¡¨è¿‡å¤§ï¼‰
        node_id = 0
        node_map = {}
        edge_set = set()  # é¿å…é‡å¤çš„è¾¹

        for i, (feature, info) in enumerate(sorted_features[:6]):
            if i >= 6:
                break

            node_id += 1
            feature_node = f"F{node_id}"
            node_map[feature] = feature_node

            # æ·»åŠ åŠŸèƒ½ç‚¹èŠ‚ç‚¹ - ä½¿ç”¨è½¬ä¹‰å‡½æ•°å¤„ç†èŠ‚ç‚¹æ–‡æœ¬
            chart += f"    B --> {feature_node}[\"{escape_mermaid_text(feature)}\"]\n"

            # æ·»åŠ å­åŠŸèƒ½ç‚¹ï¼ˆæ¯ä¸ªåŠŸèƒ½ç‚¹æœ€å¤šæ·»åŠ 3ä¸ªå­åŠŸèƒ½ï¼‰
            subfeatures = list(info["subfeatures"])[:3]
            for j, subfeature in enumerate(subfeatures):
                if j >= 3:
                    break

                node_id += 1
                subfeature_node = f"SF{node_id}"

                # åˆ›å»ºè¾¹çš„æ ‡è¯†
                edge = f"{feature_node}->{subfeature_node}"

                # é¿å…æ·»åŠ é‡å¤çš„è¾¹
                if edge not in edge_set:
                    # ä½¿ç”¨è½¬ä¹‰å‡½æ•°å¤„ç†èŠ‚ç‚¹æ–‡æœ¬
                    chart += f"    {feature_node} --> {subfeature_node}[\"{escape_mermaid_text(subfeature)}\"]\n"
                    edge_set.add(edge)
    
    # æ·»åŠ å¼‚å¸¸å¤„ç†ç¤ºä¾‹èŠ‚ç‚¹ - ä½¿ç”¨è½¬ä¹‰å‡½æ•°å¤„ç†èŠ‚ç‚¹æ–‡æœ¬
    chart += f"    C --> E1[\"{escape_mermaid_text('è¾“å…¥éªŒè¯')}\"]\n"
    chart += f"    C --> E2[\"{escape_mermaid_text('è¶…æ—¶å¤„ç†')}\"]\n"
    chart += f"    C --> E3[\"{escape_mermaid_text('å®‰å…¨æ£€æŸ¥')}\"]\n"

    # æ·»åŠ è¾¹ç•Œæµ‹è¯•ç¤ºä¾‹èŠ‚ç‚¹ - ä½¿ç”¨è½¬ä¹‰å‡½æ•°å¤„ç†èŠ‚ç‚¹æ–‡æœ¬
    chart += f"    D --> B1[\"{escape_mermaid_text('æœ€å¤§å€¼æµ‹è¯•')}\"]\n"
    chart += f"    D --> B2[\"{escape_mermaid_text('æœ€å°å€¼æµ‹è¯•')}\"]\n"
    
    # æ·»åŠ CSSç±»å®šä¹‰ï¼Œç”¨ä¸åŒé¢œè‰²è¡¨ç¤ºè¦†ç›–çŠ¶æ€
    chart += "\n    classDef covered fill:#b6d7a8,stroke:#6aa84f;\n"
    chart += "    classDef partial fill:#ffe599,stroke:#f1c232;\n"
    chart += "    classDef missing fill:#ea9999,stroke:#e06666;\n"
    
    # æ ¹æ®åŠ¨æ€åˆ†æçš„è¦†ç›–çŠ¶æ€æ·»åŠ ç±»æ ‡è®°
    covered_nodes = []
    partial_nodes = []
    missing_nodes = []
    
    # ä¸»è¦èŠ‚ç‚¹æ˜ å°„
    node_mapping = {
        "åŠŸèƒ½éªŒè¯": "B",
        "å¼‚å¸¸å¤„ç†": "C",
        "è¾¹ç•Œæµ‹è¯•": "D",
        "è¾“å…¥éªŒè¯": "E1",
        "è¶…æ—¶å¤„ç†": "E2",
        "å®‰å…¨æ£€æŸ¥": "E3",
        "æœ€å¤§å€¼æµ‹è¯•": "B1",
        "æœ€å°å€¼æµ‹è¯•": "B2"
    }
    
    # æ ¹æ®è¦†ç›–çŠ¶æ€åˆ†ç±»èŠ‚ç‚¹
    for feature, status in coverage_status.items():
        if feature in node_mapping:
            node = node_mapping[feature]
            if status == "covered":
                covered_nodes.append(node)
            elif status == "partial":
                partial_nodes.append(node)
            else:
                missing_nodes.append(node)
    
    # æ·»åŠ èŠ‚ç‚¹åˆ†ç±» - ç¡®ä¿å³ä½¿åˆ—è¡¨ä¸ºç©ºä¹Ÿæ·»åŠ ç±»å®šä¹‰
    chart += "\n"
    if covered_nodes:
        chart += f"    class {','.join(covered_nodes)} covered;\n"
    else:
        chart += "    %% æ²¡æœ‰å·²è¦†ç›–çš„èŠ‚ç‚¹\n"
    
    if partial_nodes:
        chart += f"    class {','.join(partial_nodes)} partial;\n"
    else:
        chart += "    %% æ²¡æœ‰éƒ¨åˆ†è¦†ç›–çš„èŠ‚ç‚¹\n"
    
    if missing_nodes:
        chart += f"    class {','.join(missing_nodes)} missing;\n"
    else:
        chart += "    %% æ²¡æœ‰æœªè¦†ç›–çš„èŠ‚ç‚¹\n"

    # å¼ºåˆ¶è®¾ç½®è¾¹ç•Œæµ‹è¯•ç›¸å…³èŠ‚ç‚¹çš„æ ·å¼
    if coverage_status["è¾¹ç•Œæµ‹è¯•"] == "covered":
        chart += f"    class D covered;\n"
    if coverage_status["æœ€å¤§å€¼æµ‹è¯•"] == "covered" or coverage_status["æœ€å¤§å€¼æµ‹è¯•"] == "partial":
        chart += f"    class B1 {coverage_status['æœ€å¤§å€¼æµ‹è¯•']};\n"
    if coverage_status["æœ€å°å€¼æµ‹è¯•"] == "covered" or coverage_status["æœ€å°å€¼æµ‹è¯•"] == "partial":
        chart += f"    class B2 {coverage_status['æœ€å°å€¼æµ‹è¯•']};\n"

    chart += "```\n"
    
    # æ·»åŠ å›¾ä¾‹è¯´æ˜
    chart += "\n> ğŸŸ¢ å·²è¦†ç›– | ğŸŸ¡ éƒ¨åˆ†è¦†ç›– | ğŸ”´ æœªè¦†ç›–  \n"
    
    # æ·»åŠ è¦†ç›–çŠ¶æ€æè¿°
    coverage_description = "**è¦†ç›–ç°çŠ¶**ï¼š  \n"
    
    # åˆ†ç±»è¦†ç›–çŠ¶æ€
    covered_features = []
    partial_features = []
    missing_features = []
    
    for feature, status in coverage_status.items():
        if status == "covered":
            covered_features.append(feature)
        elif status == "partial":
            partial_features.append(feature)
        else:
            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸å…³æµ‹è¯•ç”¨ä¾‹ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸åˆ—ä¸ºç¼ºå¤±é¡¹
            if feature in feature_counts and feature_counts[feature] > 0:
                missing_features.append(feature)
            # å¯¹äºå…³é”®åŠŸèƒ½ç‚¹ï¼Œå³ä½¿æ²¡æœ‰ç”¨ä¾‹ä¹Ÿæ ‡è®°ä¸ºç¼ºå¤±
            elif feature in ["åŠŸèƒ½éªŒè¯", "å¼‚å¸¸å¤„ç†", "è¾¹ç•Œæµ‹è¯•", "å®‰å…¨æ£€æŸ¥"]:
                missing_features.append(feature)
    
    # æ·»åŠ å·²è¦†ç›–åŠŸèƒ½
    if covered_features:
        coverage_description += "- ğŸŸ¢ **å·²è¦†ç›–**ï¼š" + "ã€".join(covered_features) + "  \n"
    
    # æ·»åŠ éƒ¨åˆ†è¦†ç›–åŠŸèƒ½
    if partial_features:
        coverage_description += "- ğŸŸ¡ **éƒ¨åˆ†è¦†ç›–**ï¼š" + "ã€".join(partial_features) + "  \n"
    
    # æ·»åŠ æœªè¦†ç›–åŠŸèƒ½
    if missing_features:
        coverage_description += "- ğŸ”´ **æœªè¦†ç›–**ï¼š" + "ã€".join(missing_features) + "  \n"
    
    # è¡¥å……è¯¦ç»†åˆ†æï¼Œæ ¹æ®è¯„æµ‹ç»“æœä¸­çš„æè¿°
    if evaluation_result and isinstance(evaluation_result, dict):
        if "detailed_report" in evaluation_result and "test_coverage" in evaluation_result["detailed_report"]:
            test_coverage = evaluation_result["detailed_report"]["test_coverage"]
            if "reason" in test_coverage:
                reason = test_coverage["reason"]
                if reason:
                    coverage_description += "\n**æµ‹è¯•è¦†ç›–åˆ†æ**ï¼š  \n" + reason + "  \n"
    
    # å°†è¦†ç›–çŠ¶æ€æè¿°æ·»åŠ åˆ°æµ‹è¯•è¦†ç›–å›¾åé¢
    chart += "\n" + coverage_description + "\n"
    
    # æ‰“å°è°ƒè¯•ä¿¡æ¯ï¼Œå¸®åŠ©è¯Šæ–­é—®é¢˜
    print(f"DEBUG: æµ‹è¯•ç”¨ä¾‹æ€»æ•°: {len(all_test_cases)}")
    print(f"DEBUG: åŠŸèƒ½è®¡æ•°: {feature_counts}")
    print(f"DEBUG: è¦†ç›–çŠ¶æ€: {coverage_status}")
    print(f"DEBUG: å·²è¦†ç›–èŠ‚ç‚¹: {covered_nodes}")
    print(f"DEBUG: éƒ¨åˆ†è¦†ç›–èŠ‚ç‚¹: {partial_nodes}")
    print(f"DEBUG: æœªè¦†ç›–èŠ‚ç‚¹: {missing_nodes}")

    # ä¿®å¤å›¾è¡¨è¯­æ³•ä¸­çš„åŒå¼•å·é—®é¢˜
    def fix_mermaid_chart_syntax(chart_text):
        """ä¿®å¤Mermaidå›¾è¡¨ä¸­çš„è¯­æ³•é—®é¢˜ï¼Œç‰¹åˆ«æ˜¯åŒå¼•å·ç›¸å…³é—®é¢˜"""
        # æ›¿æ¢æ‰€æœ‰ä¸­æ–‡åŒå¼•å·ä¸ºè‹±æ–‡åŒå¼•å·
        chart_text = chart_text.replace(""", "\"").replace(""", "\"")
        
        # ä¿®å¤èŠ‚ç‚¹å®šä¹‰ä¸­çš„åŒå¼•å·é—®é¢˜ - ç¡®ä¿ä½¿ç”¨è‹±æ–‡åŒå¼•å·
        chart_text = re.sub(r'(\w+)\["([^"]+)"\]', r'\1["\2"]', chart_text)
        
        return chart_text

    # åœ¨è¿”å›å›¾è¡¨å‰ä¿®å¤è¯­æ³•
    chart = fix_mermaid_chart_syntax(chart)

    return chart


async def generate_markdown_report(session: aiohttp.ClientSession, evaluation_result, is_iteration=False, formatted_ai_cases=None, formatted_prev_cases=None):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„è¯„æµ‹æŠ¥å‘Š

    :param session: aiohttpä¼šè¯
    :param evaluation_result: è¯„æµ‹ç»“æœ
    :param is_iteration: æ˜¯å¦å¯ç”¨è¿­ä»£å‰åå¯¹æ¯”åŠŸèƒ½
    :param formatted_ai_cases: æ ¼å¼åŒ–åçš„AIæµ‹è¯•ç”¨ä¾‹ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºè¿­ä»£å¯¹æ¯”
    :param formatted_prev_cases: æ ¼å¼åŒ–åçš„ä¸Šä¸€æ¬¡è¿­ä»£æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºè¿­ä»£å¯¹æ¯”
    :return: Markdownæ ¼å¼çš„æŠ¥å‘Š
    """
    log("å¼€å§‹ç”ŸæˆMarkdownæŠ¥å‘Š", important=True)
    
    # åœ¨è¿­ä»£æ¨¡å¼ä¸‹ç”Ÿæˆç²¾ç®€æŠ¥å‘Šï¼ŒåªåŒ…å«åˆå¹¶å»ºè®®å’Œæ”¹è¿›å»ºè®®
    if is_iteration:
        log("ç”ŸæˆåŒ…å«è¿­ä»£å¯¹æ¯”åˆ†æçš„ç²¾ç®€æŠ¥å‘Šï¼ŒåªåŒ…å«åˆå¹¶å»ºè®®å’Œæ”¹è¿›å»ºè®®", important=True)
        log(f"å‚æ•°æ£€æŸ¥: is_iteration={is_iteration}, formatted_ai_casesç±»å‹={type(formatted_ai_cases)}, formatted_prev_casesç±»å‹={type(formatted_prev_cases)}")
        log(f"è¯„æµ‹ç»“æœç±»å‹: {type(evaluation_result)}, æ˜¯å­—å…¸: {isinstance(evaluation_result, dict)}")
        if isinstance(evaluation_result, dict):
            log(f"è¯„æµ‹ç»“æœé”®: {', '.join(evaluation_result.keys())}")
            if "iteration_comparison" in evaluation_result:
                log("è¯„æµ‹ç»“æœä¸­åŒ…å«è¿­ä»£å¯¹æ¯”æ•°æ®")
            elif "iteration_comparison_data" in evaluation_result:
                log("è¯„æµ‹ç»“æœä¸­åŒ…å«è¿­ä»£å¯¹æ¯”æ•°æ®(data)")
            else:
                log("è­¦å‘Š: è¯„æµ‹ç»“æœä¸­ä¸åŒ…å«è¿­ä»£å¯¹æ¯”æ•°æ®", level="WARNING")
        
        # ç¡®ä¿æ—¥å¿—è®°å½•æŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ‰§è¡Œ
        await asyncio.sleep(0.05)  # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
        
        if not isinstance(evaluation_result, dict):
            return "# è¿­ä»£è¯„æµ‹æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\næ— æ³•è§£æè¯„æµ‹ç»“æœï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚"
        
        # æ£€æŸ¥å®é™…ä½¿ç”¨çš„è¯„ä¼°æ¡†æ¶
        actual_framework = "Standard"
        if "evaluation_framework" in evaluation_result:
            actual_framework = evaluation_result["evaluation_framework"]
        elif "committee_summary" in evaluation_result and "evaluation_framework" in evaluation_result["committee_summary"]:
            actual_framework = evaluation_result["committee_summary"]["evaluation_framework"]
        elif "committee_info" in evaluation_result and "evaluation_framework" in evaluation_result["committee_info"]:
            actual_framework = evaluation_result["committee_info"]["evaluation_framework"]
        
        # æ ¹æ®å®é™…è¯„ä¼°æ¡†æ¶è®¾ç½®æ ‡å¿—
        is_using_collab_eval = actual_framework == "CollabEval"
        
        # ç”Ÿæˆç²¾ç®€çš„è¿­ä»£å¯¹æ¯”æŠ¥å‘Š
        simplified_report = "# ğŸ”„ è¿­ä»£å‰åå¯¹æ¯”åˆ†ææŠ¥å‘Š\n\n"
        
        # æ·»åŠ æ€»ä½“è¯„åˆ†ä¿¡æ¯
        if "evaluation_summary" in evaluation_result:
            overall_score = evaluation_result["evaluation_summary"].get("overall_score", "N/A")
            simplified_report += f"## æ€»ä½“è¯„åˆ†\n\n**æ€»ä½“è¯„åˆ†**: {overall_score}/5.0\n\n"
            
            # æ·»åŠ æœ€ç»ˆå»ºè®®
            final_suggestion = evaluation_result["evaluation_summary"].get("final_suggestion", "æ— å»ºè®®")
            
            # æ›¿æ¢å¯èƒ½ä¸æ­£ç¡®çš„è¯„æµ‹æ¡†æ¶æè¿°
            if "ã€CollabEvalä¸‰é˜¶æ®µç»¼åˆè¯„æµ‹ã€‘" in final_suggestion and not is_using_collab_eval:
                final_suggestion = final_suggestion.replace("ã€CollabEvalä¸‰é˜¶æ®µç»¼åˆè¯„æµ‹ã€‘", "ã€å¤šè¯„å§”ç»¼åˆè¯„æµ‹ã€‘")
            elif "ã€å¤šè¯„å§”ç»¼åˆè¯„æµ‹ã€‘" in final_suggestion and is_using_collab_eval:
                final_suggestion = final_suggestion.replace("ã€å¤šè¯„å§”ç»¼åˆè¯„æµ‹ã€‘", "ã€CollabEvalä¸‰é˜¶æ®µç»¼åˆè¯„æµ‹ã€‘")
                
            simplified_report += f"## æ€»ä½“å»ºè®®\n\n{final_suggestion}\n\n"
        
        # æ·»åŠ åˆå¹¶å»ºè®®éƒ¨åˆ†
        simplified_report += "## ğŸ› ï¸ åˆå¹¶å»ºè®®\n\n"
        
        # ä»evaluation_resultä¸­æå–åˆå¹¶å»ºè®®
        merge_suggestions = []
        if "duplicate_info" in evaluation_result and "merge_suggestions" in evaluation_result["duplicate_info"]:
            merge_suggestions = evaluation_result["duplicate_info"]["merge_suggestions"]
        elif "detailed_report" in evaluation_result and "duplicate_analysis" in evaluation_result["detailed_report"]:
            if "merge_suggestions" in evaluation_result["detailed_report"]["duplicate_analysis"]:
                merge_suggestions = evaluation_result["detailed_report"]["duplicate_analysis"]["merge_suggestions"]
        
        if merge_suggestions:
            if isinstance(merge_suggestions, str):
                # å¦‚æœmerge_suggestionsæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥æ·»åŠ 
                simplified_report += merge_suggestions + "\n\n"
            elif isinstance(merge_suggestions, list) and len(merge_suggestions) > 0:
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œéå†æ·»åŠ æ¯ä¸ªåˆå¹¶å»ºè®®
                for i, suggestion in enumerate(merge_suggestions):
                    simplified_report += f"### åˆå¹¶å»ºè®® {i+1}\n\n"
                    if isinstance(suggestion, dict):
                        # æå–é‡è¦ä¿¡æ¯
                        case_ids = suggestion.get("case_ids", [])
                        case_ids_str = ", ".join(str(case_id) for case_id in case_ids[:5])
                        if len(case_ids) > 5:
                            case_ids_str += f"... ç­‰{len(case_ids)}ä¸ª"
                            
                        if "merged_case" in suggestion:
                            merged_case = suggestion["merged_case"]
                            simplified_report += f"- **æ¶‰åŠæµ‹è¯•ç”¨ä¾‹**: {case_ids_str}\n"
                            simplified_report += f"- **åˆå¹¶åæ ‡é¢˜**: {merged_case.get('title', 'æ— æ ‡é¢˜')}\n"
                            
                            # æ·»åŠ æ­¥éª¤å’Œé¢„æœŸç»“æœæ‘˜è¦
                            steps = merged_case.get("steps", [])
                            if steps and len(steps) > 0:
                                simplified_report += "- **åˆå¹¶åæ­¥éª¤**:\n"
                                for step in steps[:3]:
                                    simplified_report += f"  - {step}\n"
                                if len(steps) > 3:
                                    simplified_report += f"  - ...ç­‰{len(steps)}ä¸ªæ­¥éª¤\n"
                                    
                            expected = merged_case.get("expected_results", [])
                            if expected and len(expected) > 0:
                                simplified_report += "- **åˆå¹¶åé¢„æœŸç»“æœ**:\n"
                                for exp in expected[:3]:
                                    simplified_report += f"  - {exp}\n"
                                if len(expected) > 3:
                                    simplified_report += f"  - ...ç­‰{len(expected)}ä¸ªé¢„æœŸç»“æœ\n"
                    else:
                        simplified_report += f"{suggestion}\n"
                    
                    simplified_report += "\n"
            else:
                simplified_report += "æœªæ‰¾åˆ°éœ€è¦åˆå¹¶çš„æµ‹è¯•ç”¨ä¾‹ã€‚\n\n"
        else:
            simplified_report += "æœªæ‰¾åˆ°éœ€è¦åˆå¹¶çš„æµ‹è¯•ç”¨ä¾‹ã€‚\n\n"
        
        # æ·»åŠ æ”¹è¿›å»ºè®®éƒ¨åˆ†
        simplified_report += "## ğŸ“ æ”¹è¿›å»ºè®®\n\n"
        
        # ä»è¿­ä»£å¯¹æ¯”ä¸­æå–æ”¹è¿›å»ºè®®
        if "detailed_report" in evaluation_result and "iteration_comparison" in evaluation_result["detailed_report"]:
            iteration_comparison = evaluation_result["detailed_report"]["iteration_comparison"]
            
            # æ·»åŠ è¿­ä»£å¯¹æ¯”åˆ†æ•°
            score = iteration_comparison.get("score", "N/A")
            simplified_report += f"**è¿­ä»£æ”¹è¿›å¾—åˆ†**: {score}/5.0\n\n"
            
            # æ·»åŠ ä¸»è¦æ”¹è¿›ç‚¹
            if "key_improvements" in iteration_comparison and iteration_comparison["key_improvements"]:
                simplified_report += "### ä¸»è¦æ”¹è¿›ç‚¹\n\n"
                for improvement in iteration_comparison["key_improvements"]:
                    simplified_report += f"âœ… {improvement}\n"
                simplified_report += "\n"
                
            # æ·»åŠ ä¸»è¦é€€æ­¥ç‚¹
            if "key_regressions" in iteration_comparison and iteration_comparison["key_regressions"]:
                simplified_report += "### ä¸»è¦é€€æ­¥ç‚¹\n\n"
                for regression in iteration_comparison["key_regressions"]:
                    simplified_report += f"âš ï¸ {regression}\n"
                simplified_report += "\n"
                
            # æ·»åŠ ä¸‹ä¸€æ¬¡è¿­ä»£å»ºè®®
            if "next_iteration_suggestions" in iteration_comparison and iteration_comparison["next_iteration_suggestions"]:
                simplified_report += "### ä¸‹ä¸€æ¬¡è¿­ä»£å»ºè®®\n\n"
                for suggestion in iteration_comparison["next_iteration_suggestions"]:
                    simplified_report += f"ğŸ“ {suggestion}\n"
                simplified_report += "\n"
                
            # æ·»åŠ ç®€è¦ç†ç”±è¯´æ˜
            if "reason" in iteration_comparison:
                reason = iteration_comparison["reason"]
                # å¦‚æœç†ç”±å¤ªé•¿ï¼Œåªå–å‰300ä¸ªå­—ç¬¦
                if len(reason) > 300:
                    simplified_report += f"### ç®€è¦åˆ†æ\n\n{reason[:300]}...\n\n"
                else:
                    simplified_report += f"### ç®€è¦åˆ†æ\n\n{reason}\n\n"
        else:
            # æå–ä¸€èˆ¬æ€§æ”¹è¿›å»ºè®®
            if "evaluation_summary" in evaluation_result and "final_suggestion" in evaluation_result["evaluation_summary"]:
                suggestion = evaluation_result["evaluation_summary"]["final_suggestion"]
                simplified_report += f"{suggestion}\n\n"
            else:
                simplified_report += "æ— å…·ä½“æ”¹è¿›å»ºè®®ã€‚\n\n"
        
        # æ·»åŠ é‡å¤ç‡ä¿¡æ¯
        if "duplicate_info" in evaluation_result:
            duplicate_info = evaluation_result["duplicate_info"]
            ai_duplicate_rate = duplicate_info.get("ai_duplicate_rate", 0)
            
            # å¦‚æœè¿­ä»£å¯¹æ¯”æ•°æ®å¯ç”¨ï¼Œæ·»åŠ é‡å¤ç‡å˜åŒ–
            if "iteration_comparison_data" in evaluation_result:
                iteration_data = evaluation_result["iteration_comparison_data"]
                prev_duplicate_rate = iteration_data.get("prev_duplicate_rate", 0)
                duplicate_rate_change = ai_duplicate_rate - prev_duplicate_rate
                
                simplified_report += "## ğŸ“Š é‡å¤ç‡åˆ†æ\n\n"
                simplified_report += f"- **å½“å‰è¿­ä»£é‡å¤ç‡**: {ai_duplicate_rate}%\n"
                simplified_report += f"- **ä¸Šä¸€æ¬¡è¿­ä»£é‡å¤ç‡**: {prev_duplicate_rate}%\n"
                simplified_report += f"- **å˜åŒ–**: {'+' if duplicate_rate_change > 0 else ''}{duplicate_rate_change:.2f}ä¸ªç™¾åˆ†ç‚¹\n\n"
        
        # æ·»åŠ é¡µè„š
        from datetime import datetime
        # ç¡®ä¿ä½¿ç”¨å®æ—¶æ—¶é—´
        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

        # æ„å»ºé¡µè„š
        footer = f"**ç”Ÿæˆæ—¶é—´ï¼š{current_time} â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**"
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é¡µè„šï¼Œå¦‚æœæœ‰åˆ™æ›¿æ¢ï¼Œå¦åˆ™æ·»åŠ 
        import re
        footer_pattern = r"\*\*ç”Ÿæˆæ—¶é—´ï¼š(.*?)(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
        placeholder_patterns = [
            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
            r"\*\*ç”Ÿæˆæ—¶é—´ï¼š<.*?>(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
        ]
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„å ä½ç¬¦
        placeholder_found = False
        for pattern in placeholder_patterns:
            if re.search(pattern, simplified_report):
                report = re.sub(pattern, footer, simplified_report)
                placeholder_found = True
                log("å·²æ›¿æ¢åŸºæœ¬æŠ¥å‘Šé¡µè„šä¸­çš„æ˜ç¡®å ä½ç¬¦ä¸ºå®æ—¶æ—¶é—´", important=True)
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å ä½ç¬¦ï¼Œå°è¯•ä½¿ç”¨é€šç”¨æ¨¡å¼
        if not placeholder_found and re.search(footer_pattern, simplified_report):
            report = re.sub(footer_pattern, footer, simplified_report)
            log("å·²æ›¿æ¢åŸºæœ¬æŠ¥å‘Šé¡µè„šä¸­çš„æ—¥æœŸä¸ºå®æ—¶æ—¶é—´", important=True)
        elif not placeholder_found:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¡µè„šï¼Œåˆ™æ·»åŠ åˆ°æŠ¥å‘Šæœ«å°¾
            report += f"\n\n---\n{footer}\n"
            log("æœªæ‰¾åˆ°é¡µè„šï¼Œå·²æ·»åŠ å¸¦æœ‰å®æ—¶æ—¶é—´çš„é¡µè„šåˆ°åŸºæœ¬æŠ¥å‘Š", important=True)
        
        log(f"è¿­ä»£å¯¹æ¯”ç²¾ç®€æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(simplified_report)} å­—ç¬¦", important=True)
        return simplified_report
    
    # ä»¥ä¸‹æ˜¯åŸæœ‰çš„å®Œæ•´æŠ¥å‘Šç”Ÿæˆé€»è¾‘ï¼Œä»…åœ¨éè¿­ä»£æ¨¡å¼ä¸‹æ‰§è¡Œ
    if is_iteration and formatted_prev_cases:
        log("ç”ŸæˆåŒ…å«è¿­ä»£å¯¹æ¯”åˆ†æçš„æŠ¥å‘Š", important=True)

    # ç¡®ä¿æ—¥å¿—è®°å½•æŒ‰ç…§æ­£ç¡®çš„é¡ºåºæ‰§è¡Œ
    await asyncio.sleep(0.1)  # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº

    # ä»è¯„ä¼°ç»“æœä¸­æå–å…³é”®æ•°æ®ç”¨äºå¯è§†åŒ–
    mermaid_data = {
        "scores": {},
        "duplicate_rates": {
            "ai": 0,
            "golden": 0
        },
        "duplicate_types": {},
        "coverage": []
    }

    # å°è¯•ä»è¯„ä¼°ç»“æœè·å–æµ‹è¯•ç”¨ä¾‹æ•°æ®
    ai_testcases = []
    if isinstance(evaluation_result, dict):
        # å°è¯•ä»ä¸åŒå¯èƒ½çš„å­—æ®µä¸­æå–æµ‹è¯•ç”¨ä¾‹
        if "test_cases" in evaluation_result:
            ai_testcases = evaluation_result.get("test_cases", [])
        elif "detailed_report" in evaluation_result and "test_coverage" in evaluation_result["detailed_report"]:
            if "analysis" in evaluation_result["detailed_report"]["test_coverage"]:
                coverage_analysis = evaluation_result["detailed_report"]["test_coverage"]["analysis"]
                # å°è¯•ä»è¦†ç›–åˆ†æä¸­æå–åŸºæœ¬ä¿¡æ¯ç”¨äºç”Ÿæˆæµç¨‹å›¾
                if "covered_features" in coverage_analysis:
                    covered_features = coverage_analysis["covered_features"]
                    # å°†è¦†ç›–çš„åŠŸèƒ½ç‚¹è½¬æ¢ä¸ºç®€å•çš„æµ‹è¯•ç”¨ä¾‹ç»“æ„
                    ai_testcases = [{"case_id": f"FEAT-{i + 1}", "title": feature}
                                    for i, feature in enumerate(covered_features)]

    # å¦‚æœai_testcasesä¸ºç©ºï¼Œä½†æœ‰formatted_ai_casesï¼Œåˆ™ä½¿ç”¨formatted_ai_cases
    if not ai_testcases and formatted_ai_cases:
        log("ä½¿ç”¨formatted_ai_casesä½œä¸ºæµ‹è¯•ç”¨ä¾‹æ•°æ®æº", important=True)
        # å¤„ç†ä¸åŒæ ¼å¼çš„formatted_ai_cases
        if isinstance(formatted_ai_cases, dict):
            if "testcases" in formatted_ai_cases and "test_cases" in formatted_ai_cases["testcases"]:
                ai_testcases = formatted_ai_cases["testcases"]["test_cases"]
            elif "test_cases" in formatted_ai_cases:
                ai_testcases = formatted_ai_cases["test_cases"]
            # å¤„ç†åŸå§‹æ ¼å¼çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆå¦‚åŒ…å«functionalã€securityç­‰åˆ†ç±»çš„å­—å…¸ï¼‰
            else:
                log("æ£€æµ‹åˆ°åŸå§‹æ ¼å¼çš„æµ‹è¯•ç”¨ä¾‹ï¼Œç›´æ¥ä¼ é€’ç»™è¦†ç›–ç‡åˆ†æå‡½æ•°", important=True)
                ai_testcases = formatted_ai_cases

        log(f"æå–çš„æµ‹è¯•ç”¨ä¾‹æ•°æ®ç±»å‹: {type(ai_testcases)}", important=True)
        if isinstance(ai_testcases, dict):
            log(f"æµ‹è¯•ç”¨ä¾‹å­—å…¸åŒ…å«çš„é”®: {', '.join(ai_testcases.keys())}", important=True)
        elif isinstance(ai_testcases, list):
            log(f"æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨é•¿åº¦: {len(ai_testcases)}", important=True)

    # åŠ¨æ€ç”Ÿæˆæµ‹è¯•è¦†ç›–æµç¨‹å›¾
    coverage_chart = generate_test_coverage_flow_chart(ai_testcases, evaluation_result)

    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œç”Ÿæˆè¿­ä»£å¯¹æ¯”å›¾è¡¨
    iteration_comparison_chart = ""
    if is_iteration and formatted_prev_cases and formatted_ai_cases and isinstance(evaluation_result, dict):
        log("ç”Ÿæˆè¿­ä»£å¯¹æ¯”å›¾è¡¨", important=True)
        
        # æå–å½“å‰è¿­ä»£å’Œä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹
        current_testcases = []
        prev_testcases = []

        # æå–å½“å‰è¿­ä»£æµ‹è¯•ç”¨ä¾‹
        if formatted_ai_cases:
            if "testcases" in formatted_ai_cases and "test_cases" in formatted_ai_cases["testcases"]:
                current_testcases = formatted_ai_cases["testcases"]["test_cases"]

        # æå–ä¸Šä¸€æ¬¡è¿­ä»£æµ‹è¯•ç”¨ä¾‹
        if formatted_prev_cases:
            if "testcases" in formatted_prev_cases and "test_cases" in formatted_prev_cases["testcases"]:
                prev_testcases = formatted_prev_cases["testcases"]["test_cases"]

        # ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹æ•°é‡å¯¹æ¯”å›¾è¡¨
        count_chart = "## ğŸ“Š è¿­ä»£æµ‹è¯•ç”¨ä¾‹æ•°é‡å¯¹æ¯”\n\n"
        prev_count = len(prev_testcases) if isinstance(prev_testcases, list) else 0
        current_count = len(current_testcases) if isinstance(current_testcases, list) else 0
        count_change = current_count - prev_count
        count_change_percent = round((count_change / prev_count * 100) if prev_count > 0 else 0, 2)

        # ç”Ÿæˆæ•°é‡å¯¹æ¯”æ¡å½¢å›¾
        count_chart += "```mermaid\nbar\n"
        count_chart += "    title æµ‹è¯•ç”¨ä¾‹æ•°é‡å˜åŒ–\n"
        count_chart += "    xlabel è¿­ä»£ç‰ˆæœ¬\n"
        count_chart += "    ylabel æµ‹è¯•ç”¨ä¾‹æ•°é‡\n"
        count_chart += f"    \"ä¸Šä¸€æ¬¡è¿­ä»£\" {prev_count}\n"
        count_chart += f"    \"å½“å‰è¿­ä»£\" {current_count}\n"
        count_chart += "```\n\n"

        # æ·»åŠ æ•°é‡å˜åŒ–æè¿°
        count_chart += f"### æ•°é‡å˜åŒ–åˆ†æ\n\n"

        if count_change > 0:
            count_chart += f"ğŸ“ˆ **å¢åŠ **: +{count_change}ä¸ªæµ‹è¯•ç”¨ä¾‹ (+{count_change_percent}%)\n\n"
        elif count_change < 0:
            count_chart += f"ğŸ“‰ **å‡å°‘**: {count_change}ä¸ªæµ‹è¯•ç”¨ä¾‹ ({count_change_percent}%)\n\n"
        else:
            count_chart += f"ğŸ“Š **æ— å˜åŒ–**: æµ‹è¯•ç”¨ä¾‹æ•°é‡ä¿æŒä¸å˜\n\n"

        # ä»è¯„ä¼°ç»“æœä¸­æå–é‡å¤ç‡æ•°æ®
        if "duplicate_info" in evaluation_result:
            duplicate_info = evaluation_result["duplicate_info"]
            current_duplicate_rate = duplicate_info.get("ai_duplicate_rate", 0)

            # ä»è¿­ä»£å¯¹æ¯”æ•°æ®ä¸­æå–ä¸Šä¸€æ¬¡è¿­ä»£çš„é‡å¤ç‡
            prev_duplicate_rate = 0
            if "iteration_comparison_data" in evaluation_result:
                iteration_data = evaluation_result["iteration_comparison_data"]
                prev_duplicate_rate = iteration_data.get("prev_duplicate_rate", 0)

            # è®¡ç®—é‡å¤ç‡å˜åŒ–
            duplicate_rate_change = current_duplicate_rate - prev_duplicate_rate

            # ç”Ÿæˆé‡å¤ç‡å¯¹æ¯”å›¾è¡¨
            count_chart += "### é‡å¤ç‡å˜åŒ–\n\n"
            count_chart += "```mermaid\nbar\n"
            count_chart += "    title æµ‹è¯•ç”¨ä¾‹é‡å¤ç‡å˜åŒ–\n"
            count_chart += "    xlabel è¿­ä»£ç‰ˆæœ¬\n"
            count_chart += "    ylabel é‡å¤ç‡(%)\n"
            count_chart += f"    \"ä¸Šä¸€æ¬¡è¿­ä»£\" {prev_duplicate_rate}\n"
            count_chart += f"    \"å½“å‰è¿­ä»£\" {current_duplicate_rate}\n"
            count_chart += "```\n\n"

            # æ·»åŠ é‡å¤ç‡å˜åŒ–æè¿°
            if duplicate_rate_change > 0:
                count_chart += f"âš ï¸ **å¢åŠ **: +{duplicate_rate_change:.2f}ä¸ªç™¾åˆ†ç‚¹\n\n"
            elif duplicate_rate_change < 0:
                count_chart += f"âœ… **å‡å°‘**: {duplicate_rate_change:.2f}ä¸ªç™¾åˆ†ç‚¹\n\n"
            else:
                count_chart += f"ğŸ“Š **æ— å˜åŒ–**: é‡å¤ç‡ä¿æŒä¸å˜\n\n"

        # ä»è¯„ä¼°ç»“æœä¸­æå–è¿­ä»£å¯¹æ¯”åˆ†ææ•°æ®
        if "detailed_report" in evaluation_result and "iteration_comparison" in evaluation_result["detailed_report"]:
            iteration_comparison = evaluation_result["detailed_report"]["iteration_comparison"]

            # æ·»åŠ è¿­ä»£æ”¹è¿›å¾—åˆ†
            improvement_score = iteration_comparison.get("score", "N/A")
            count_chart += f"### è¿­ä»£æ”¹è¿›å¾—åˆ†: {improvement_score}/5.0\n\n"

            # æ·»åŠ ä¸»è¦æ”¹è¿›ç‚¹
            if "key_improvements" in iteration_comparison and iteration_comparison["key_improvements"]:
                count_chart += "### ä¸»è¦æ”¹è¿›ç‚¹\n\n"
                for improvement in iteration_comparison["key_improvements"]:
                    count_chart += f"âœ… {improvement}\n"
                count_chart += "\n"

            # æ·»åŠ ä¸»è¦é€€æ­¥ç‚¹
            if "key_regressions" in iteration_comparison and iteration_comparison["key_regressions"]:
                count_chart += "### ä¸»è¦é€€æ­¥ç‚¹\n\n"
                for regression in iteration_comparison["key_regressions"]:
                    count_chart += f"âš ï¸ {regression}\n"
                count_chart += "\n"

            # æ·»åŠ ä¸‹ä¸€æ¬¡è¿­ä»£å»ºè®®
            if "next_iteration_suggestions" in iteration_comparison and iteration_comparison["next_iteration_suggestions"]:
                count_chart += "### ä¸‹ä¸€æ¬¡è¿­ä»£å»ºè®®\n\n"
                for suggestion in iteration_comparison["next_iteration_suggestions"]:
                    count_chart += f"ğŸ“ {suggestion}\n"
                count_chart += "\n"

            # æ·»åŠ è¿­ä»£å¯¹æ¯”ç†ç”±
            if "reason" in iteration_comparison:
                count_chart += "### è¯¦ç»†åˆ†æ\n\n"
                count_chart += f"{iteration_comparison['reason']}\n\n"

        # å°†è¿­ä»£å¯¹æ¯”å›¾è¡¨æ·»åŠ åˆ°æ€»çš„è¿­ä»£å¯¹æ¯”å›¾è¡¨ä¸­
        iteration_comparison_chart = "# ğŸ”„ è¿­ä»£å‰åå¯¹æ¯”åˆ†æ\n\n" + count_chart

    # æ·»åŠ æµ‹è¯•è¦†ç›–çŠ¶æ€çš„å®é™…æ–‡å­—æè¿°
    # ä»è¯„ä¼°ç»“æœä¸­æå–è¦†ç›–åº¦ç›¸å…³ä¿¡æ¯
    coverage_status = {
        "åŠŸèƒ½éªŒè¯": "missing",
        "å¼‚å¸¸å¤„ç†": "missing",
        "è¾¹ç•Œæµ‹è¯•": "missing",
        "è¾“å…¥éªŒè¯": "missing",
        "è¶…æ—¶å¤„ç†": "missing",
        "å®‰å…¨æ£€æŸ¥": "missing",
        "æœ€å¤§å€¼æµ‹è¯•": "missing",
        "æœ€å°å€¼æµ‹è¯•": "missing"
    }

    # å¦‚æœæœ‰è¯„ä¼°ç»“æœï¼Œä»ä¸­æå–ä¿¡æ¯å¢å¼ºè¦†ç›–åˆ†æ
    if evaluation_result and isinstance(evaluation_result, dict):
        # ä»è¯„æµ‹ç»“æœä¸­åˆ†æå„ç»´åº¦åˆ†æ•°ï¼Œæ¨æ–­è¦†ç›–çŠ¶æ€
        detailed = evaluation_result.get("detailed_report", {})

        # åŠŸèƒ½è¦†ç›–åº¦è¯„åˆ†å½±å“"åŠŸèƒ½éªŒè¯"çŠ¶æ€
        try:
            functional_score = float(detailed.get("functional_coverage", {}).get("score", 0))
            if functional_score >= 4.0:
                coverage_status["åŠŸèƒ½éªŒè¯"] = "covered"
            elif functional_score >= 3.0:
                coverage_status["åŠŸèƒ½éªŒè¯"] = "partial"

            # ç¼ºé™·å‘ç°èƒ½åŠ›è¯„åˆ†å½±å“"å¼‚å¸¸å¤„ç†"å’Œ"å®‰å…¨æ£€æŸ¥"çŠ¶æ€
            defect_score = float(detailed.get("defect_detection", {}).get("score", 0))
            if defect_score >= 4.0:
                coverage_status["å¼‚å¸¸å¤„ç†"] = "covered"
            elif defect_score >= 3.0:
                coverage_status["å¼‚å¸¸å¤„ç†"] = "partial"

            # æµ‹è¯•è¦†ç›–åº¦è¯„åˆ†å½±å“"è¾¹ç•Œæµ‹è¯•"çŠ¶æ€
            test_coverage_score = float(detailed.get("test_coverage", {}).get("score", 0))
            if test_coverage_score >= 4.0:
                coverage_status["è¾¹ç•Œæµ‹è¯•"] = "covered"
            elif test_coverage_score >= 3.0:
                coverage_status["è¾¹ç•Œæµ‹è¯•"] = "partial"

            # å®‰å…¨ä¸ç»æµæ€§è¯„åˆ†å½±å“"å®‰å…¨æ£€æŸ¥"çŠ¶æ€
            security_score = float(detailed.get("security_economy", {}).get("score", 0))
            if security_score >= 4.0:
                coverage_status["å®‰å…¨æ£€æŸ¥"] = "covered"
            elif security_score >= 3.0:
                coverage_status["å®‰å…¨æ£€æŸ¥"] = "partial"
        except (ValueError, TypeError):
            # å¦‚æœè¯„åˆ†æ— æ³•è½¬æ¢ä¸ºæµ®ç‚¹æ•°ï¼Œè·³è¿‡
            pass

    # æå–åŠŸèƒ½è®¡æ•°
    feature_counts = {}
    for case in ai_testcases:
        title = case.get("title", "").lower()
        case_id = case.get("case_id", "")

        # åŸºäºæµ‹è¯•ç”¨ä¾‹æ ‡é¢˜å’ŒIDåˆ†æè¦†ç›–ç±»å‹
        if "åŠŸèƒ½" in title or "æµç¨‹" in title or "FUNC" in case_id:
            feature_counts["åŠŸèƒ½éªŒè¯"] = feature_counts.get("åŠŸèƒ½éªŒè¯", 0) + 1

        if "å¼‚å¸¸" in title or "é”™è¯¯" in title or "EXCEP" in case_id:
            feature_counts["å¼‚å¸¸å¤„ç†"] = feature_counts.get("å¼‚å¸¸å¤„ç†", 0) + 1

        if "è¾¹ç•Œ" in title or "æé™" in title or "BOUND" in case_id:
            feature_counts["è¾¹ç•Œæµ‹è¯•"] = feature_counts.get("è¾¹ç•Œæµ‹è¯•", 0) + 1

        if "è¾“å…¥" in title and ("éªŒè¯" in title or "æ ¡éªŒ" in title):
            feature_counts["è¾“å…¥éªŒè¯"] = feature_counts.get("è¾“å…¥éªŒè¯", 0) + 1

        if "è¶…æ—¶" in title or "timeout" in title.lower():
            feature_counts["è¶…æ—¶å¤„ç†"] = feature_counts.get("è¶…æ—¶å¤„ç†", 0) + 1

        if "å®‰å…¨" in title or "æ”»å‡»" in title or "æ¼æ´" in title or "SEC" in case_id:
            feature_counts["å®‰å…¨æ£€æŸ¥"] = feature_counts.get("å®‰å…¨æ£€æŸ¥", 0) + 1

        if "æœ€å¤§" in title or "ä¸Šé™" in title or "max" in title.lower():
            feature_counts["æœ€å¤§å€¼æµ‹è¯•"] = feature_counts.get("æœ€å¤§å€¼æµ‹è¯•", 0) + 1

        if "æœ€å°" in title or "ä¸‹é™" in title or "min" in title.lower():
            feature_counts["æœ€å°å€¼æµ‹è¯•"] = feature_counts.get("æœ€å°å€¼æµ‹è¯•", 0) + 1

    # æ ¹æ®åŠŸèƒ½è®¡æ•°è°ƒæ•´è¦†ç›–çŠ¶æ€
    # å¦‚æœè®¡æ•°ä¸º0ï¼Œä¿æŒä¸º"missing"
    # å¦‚æœè®¡æ•°å¤§äº0ä½†è¯„ä¼°ç»“æœæœªè¦†ç›–ï¼Œè®¾ä¸º"partial"
    for feature, count in feature_counts.items():
        if count > 0 and coverage_status[feature] == "missing":
            coverage_status[feature] = "partial"

    # æå–å„ç»´åº¦è¯„åˆ†
    if isinstance(evaluation_result, dict) and "detailed_report" in evaluation_result:
        detailed = evaluation_result["detailed_report"]

        # æå–è¯„åˆ†æ•°æ®
        for key, value in detailed.items():
            if isinstance(value, dict) and "score" in value:
                score_key = key.replace("_", " ").title()
                try:
                    score_value = float(value["score"])
                    mermaid_data["scores"][score_key] = score_value
                except (ValueError, TypeError):
                    mermaid_data["scores"][score_key] = value["score"]

    # è·å–æ•´ä½“è¯„åˆ†
    overall_score = "N/A"
    if isinstance(evaluation_result, dict) and "evaluation_summary" in evaluation_result:
        overall_score = evaluation_result["evaluation_summary"].get("overall_score", "N/A")
        mermaid_data["scores"]["Overall Score"] = overall_score

    # ç”Ÿæˆè¯„åˆ†é›·è¾¾å›¾
    # ç”±äºMermaidä¸æ”¯æŒçœŸæ­£çš„é›·è¾¾å›¾ï¼Œæ”¹ç”¨Markdownè¡¨æ ¼å’Œè¯„åˆ†è¡¨ç¤º
    radar_chart = f"## ğŸ“Š ç»¼åˆè¯„åˆ† (æ€»ä½“: {overall_score}/5.0)\n\n"
    radar_chart += "| è¯„ä¼°ç»´åº¦ | å¾—åˆ† | è¯„åˆ†å¯è§†åŒ– |\n"
    radar_chart += "|---------|------|------------|\n"

    # è·å–ç»´åº¦æ•°æ®
    dimension_scores = []
    for name, score in mermaid_data["scores"].items():
        if name != "Overall Score":  # æ’é™¤æ€»ä½“è¯„åˆ†
            try:
                # ç¡®ä¿åˆ†æ•°æ˜¯æ•°å€¼
                score_value = float(score) if isinstance(score, str) else score
                # å°†è‹±æ–‡ç»´åº¦åç§°è½¬ä¸ºä¸­æ–‡
                chinese_name = name
                if name == "Format Compliance":
                    chinese_name = "æ ¼å¼åˆè§„æ€§"
                elif name == "Content Accuracy":
                    chinese_name = "å†…å®¹å‡†ç¡®æ€§"
                elif name == "Test Coverage":
                    chinese_name = "æµ‹è¯•è¦†ç›–åº¦"
                elif name == "Functional Coverage":
                    chinese_name = "åŠŸèƒ½è¦†ç›–åº¦"
                elif name == "Defect Detection":
                    chinese_name = "ç¼ºé™·å‘ç°èƒ½åŠ›"
                elif name == "Engineering Efficiency":
                    chinese_name = "å·¥ç¨‹æ•ˆç‡"
                elif name == "Semantic Quality":
                    chinese_name = "è¯­ä¹‰è´¨é‡"
                elif name == "Security Economy":
                    chinese_name = "å®‰å…¨ä¸ç»æµæ€§"
                elif name == "Duplicate Analysis":
                    chinese_name = "é‡å¤æ€§åˆ†æ"
                dimension_scores.append((chinese_name, score_value))
            except (ValueError, TypeError):
                # å¦‚æœæ— æ³•è½¬æ¢ä¸ºæ•°å€¼ï¼Œè·³è¿‡
                continue

    # æŒ‰è¯„åˆ†ä»é«˜åˆ°ä½æ’åº
    dimension_scores.sort(key=lambda x: x[1], reverse=True)

    # æ·»åŠ æ•°æ®è¡Œ
    if dimension_scores:
        for name, score in dimension_scores:
            # ç”Ÿæˆè¯„åˆ†å¯è§†åŒ–
            score_int = int(score)
            stars = "â˜…" * score_int + "â˜†" * (5 - score_int)
            radar_chart += f"| {name} | {score} | {stars} |\n"

    radar_chart += "\n"

    # æ·»åŠ ä¸“é—¨çš„è¯„åˆ†å›¾
    radar_chart += "```mermaid\npie\n    title å„ç»´åº¦è¯„åˆ†åˆ†å¸ƒ\n"
    for name, score in dimension_scores:
        short_name = name.replace("Coverage", "è¦†ç›–").replace("Analysis", "åˆ†æ")
        radar_chart += f"    \"{short_name}\" : {score}\n"
    radar_chart += "```\n\n"

    # æå–é‡å¤ç‡å’Œé‡å¤ç±»å‹æ•°æ®
    ai_duplicate_rate = 0
    golden_duplicate_rate = 0
    duplicate_types = {}

    # å°è¯•ä»è¯„ä¼°ç»“æœä¸­æ‰¾åˆ°é‡å¤ç‡æ•°æ®
    if "duplicate_types" in evaluation_result:
        duplicate_types = evaluation_result.get("duplicate_types", {})

        # ä»evaluation_resultç›´æ¥è·å–é‡å¤ç‡æ•°æ®
        try:
            # å°è¯•ä»å…·ä½“æ•°æ®ä¸­æå–é‡å¤ç‡
            duplicate_info = evaluation_result.get("duplicate_info", {})
            if duplicate_info:
                ai_duplicate_rate = duplicate_info.get("ai_duplicate_rate", 0)
                golden_duplicate_rate = duplicate_info.get("golden_duplicate_rate", 0)
                mermaid_data["duplicate_rates"]["ai"] = float(ai_duplicate_rate)
                mermaid_data["duplicate_rates"]["golden"] = float(golden_duplicate_rate)
        except:
            # å¦‚æœæå–å¤±è´¥ï¼Œä¿ç•™åˆå§‹åŒ–å€¼
            pass
    else:
        # å°è¯•ä»åŸå› æè¿°ä¸­æå–æ•°æ®
        if "duplicate_analysis" in evaluation_result.get("detailed_report", {}):
            dup_analysis = evaluation_result["detailed_report"]["duplicate_analysis"]
            if "reason" in dup_analysis:
                # å°è¯•ä»åŸå› æè¿°ä¸­æå–æ•°å­—
                ai_rates = re.findall(r"AI[^0-9]*([0-9.]+)%", dup_analysis["reason"])
                golden_rates = re.findall(r"é»„é‡‘[^0-9]*([0-9.]+)%", dup_analysis["reason"])

                if ai_rates:
                    mermaid_data["duplicate_rates"]["ai"] = float(ai_rates[0])
                    ai_duplicate_rate = float(ai_rates[0])
                if golden_rates:
                    mermaid_data["duplicate_rates"]["golden"] = float(golden_rates[0])
                    golden_duplicate_rate = float(golden_rates[0])

    # å°è¯•ä»è¯„ä¼°ç»“æœä¸­æå–é‡å¤ç±»å‹æ•°æ®
    dup_types = {"æ ‡é¢˜é‡å¤": 0, "æ­¥éª¤é‡å¤": 0, "é¢„æœŸç»“æœé‡å¤": 0, "æ··åˆé‡å¤": 0}

    # å¦‚æœevaluation_resultä¸­æœ‰å…·ä½“çš„duplicate_typesæ•°æ®ï¼Œåˆ™ä½¿ç”¨å®ƒ
    if "duplicate_types" in evaluation_result:
        try:
            duplicate_types = evaluation_result.get("duplicate_types", {})
            if duplicate_types and sum(duplicate_types.values()) > 0:
                dup_types = {
                    "æ ‡é¢˜é‡å¤": duplicate_types.get("title", 0),
                    "æ­¥éª¤é‡å¤": duplicate_types.get("steps", 0),
                    "é¢„æœŸç»“æœé‡å¤": duplicate_types.get("expected_results", 0),
                    "æ··åˆé‡å¤": duplicate_types.get("mixed", 0)
                }
                # ä¿å­˜åˆ°mermaid_data
                mermaid_data["duplicate_types"] = dup_types
        except:
            pass
    else:
        # å°è¯•ä»åŸå› æè¿°ä¸­æå–é‡å¤ç±»å‹åˆ†å¸ƒ
        if "duplicate_analysis" in evaluation_result.get("detailed_report", {}):
            reason = evaluation_result["detailed_report"]["duplicate_analysis"].get("reason", "")

            # å°è¯•ä»reasonä¸­æå–æ•°æ®
            title_dup = re.findall(r"æ ‡é¢˜é‡å¤[^0-9]*([0-9]+)ä¸ª", reason)
            steps_dup = re.findall(r"æ­¥éª¤[ç›¸ä¼¼|é‡å¤][^0-9]*([0-9]+)ä¸ª", reason)

            if title_dup:
                dup_types["æ ‡é¢˜é‡å¤"] = int(title_dup[0])
            if steps_dup:
                dup_types["æ­¥éª¤é‡å¤"] = int(steps_dup[0])

            # ä¿å­˜åˆ°mermaid_data
            mermaid_data["duplicate_types"] = dup_types

    # åˆå¹¶ç”Ÿæˆé‡å¤æµ‹è¯•ç”¨ä¾‹åˆ†æå›¾
    duplicate_combined_chart = "## ğŸ”„ é‡å¤æµ‹è¯•ç”¨ä¾‹åˆ†æ\n\n"

    # ä½¿ç”¨æ–‡å­—æè¿°æ›¿ä»£å›¾è¡¨
    duplicate_combined_chart += "> ### é‡å¤æƒ…å†µç»Ÿè®¡æ‘˜è¦\n>\n"

    # æ·»åŠ é‡å¤ç‡æ•°æ®
    duplicate_combined_chart += f"> **AIæµ‹è¯•ç”¨ä¾‹é‡å¤ç‡**: {ai_duplicate_rate}%\n>\n"
    duplicate_combined_chart += f"> **é»„é‡‘æ ‡å‡†é‡å¤ç‡**: {golden_duplicate_rate}%\n>\n"

    # æ·»åŠ é‡å¤ç±»å‹æ•°æ®
    duplicate_combined_chart += "> **é‡å¤ç±»å‹æ˜ç»†**:\n"
    has_duplicates = False
    for dup_type, count in dup_types.items():
        if count > 0:
            has_duplicates = True
            duplicate_combined_chart += f"> - {dup_type}: **{count}ä¸ª**\n"

    # å¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯0ï¼Œæ·»åŠ æ— é‡å¤è¯´æ˜
    if not has_duplicates:
        duplicate_combined_chart += "> - æœªå‘ç°é‡å¤æµ‹è¯•ç”¨ä¾‹\n"

    # æ·»åŠ æ¨¡å—åˆ†å¸ƒçš„æ–‡å­—æè¿°
    if "duplicate_categories" in evaluation_result:
        duplicate_categories = evaluation_result.get("duplicate_categories", {})
        if duplicate_categories:
            duplicate_combined_chart += ">\n> **é‡å¤ç”¨ä¾‹æ¨¡å—åˆ†å¸ƒ**:\n"
            for category, value in duplicate_categories.items():
                # æ£€æŸ¥valueæ˜¯å¦ä¸ºå­—å…¸ï¼ˆanalyzer.pyä¸­çš„ç»“æ„ï¼‰æˆ–æ•´æ•°
                if isinstance(value, dict) and "total" in value:
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œæå–duplicate_rateæˆ–è®¡ç®—é‡å¤ç‡
                    duplicate_count = value.get("title_duplicates", 0) + value.get("steps_duplicates", 0)
                    if duplicate_count > 0:
                        duplicate_combined_chart += f"> - {category}: **{duplicate_count}ä¸ª**\n"
                elif isinstance(value, (int, float)) and value > 0:
                    # å¦‚æœæ˜¯æ•°å­—ä¸”å¤§äº0
                    duplicate_combined_chart += f"> - {category}: **{value}ä¸ª**\n"
                # å¿½ç•¥å…¶ä»–ç±»å‹æˆ–é›¶å€¼

    duplicate_combined_chart += "\n\n"

    # ç”Ÿæˆåˆå¹¶å»ºè®®æ–¹æ¡ˆå›¾
    merge_suggestions = []
    if isinstance(evaluation_result, dict) and "detailed_report" in evaluation_result:
        detailed = evaluation_result["detailed_report"]
        if "duplicate_analysis" in detailed and "merge_suggestions" in detailed["duplicate_analysis"]:
            merge_suggestions = detailed["duplicate_analysis"]["merge_suggestions"]

    # ä»duplicate_infoä¸­è·å–åˆå¹¶å»ºè®®
    if "duplicate_info" in evaluation_result and "merge_suggestions" in evaluation_result["duplicate_info"]:
        merge_suggestions = evaluation_result["duplicate_info"]["merge_suggestions"]

    # å¦‚æœæœ‰åˆå¹¶å»ºè®®ï¼Œç”Ÿæˆå›¾è¡¨
    if merge_suggestions and isinstance(merge_suggestions, str) and len(merge_suggestions) > 10:
        # å¦‚æœmerge_suggestionsæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æå–æœ‰ç”¨ä¿¡æ¯
        merge_chart = "### ğŸ› ï¸ åˆå¹¶å»ºè®®æ–¹æ¡ˆ\n\n"
        merge_chart += "> " + merge_suggestions.replace("\n", "\n> ") + "\n\n"
    elif merge_suggestions and (isinstance(merge_suggestions, list) and len(merge_suggestions) > 0):
        # å¦‚æœæœ‰ç»“æ„åŒ–çš„åˆå¹¶å»ºè®®ï¼Œç”Ÿæˆæµç¨‹å›¾
        merge_chart = "### ğŸ› ï¸ åˆå¹¶å»ºè®®æ–¹æ¡ˆ\n```mermaid\ngraph LR\n"
        merge_chart += "    A[é‡å¤ç”¨ä¾‹] --> B[åˆå¹¶æ–¹æ¡ˆ]\n"

        for i, suggestion in enumerate(merge_suggestions[:4]):  # é™åˆ¶æœ€å¤šæ˜¾ç¤º4ä¸ªå»ºè®®
            index = i + 1
            case_ids = ""
            title = ""
            node_id = ""  # ç”¨äºä¿å­˜èŠ‚ç‚¹ID

            if isinstance(suggestion, dict):
                # æå–æ¡ˆä¾‹IDå¹¶ç”ŸæˆèŠ‚ç‚¹ID
                all_case_ids = []
                if "original_case_ids" in suggestion:
                    # ä¼˜å…ˆä½¿ç”¨åŸå§‹case_ids
                    all_case_ids = suggestion["original_case_ids"]
                elif "case_ids" in suggestion and suggestion["case_ids"]:
                    # å¦‚æœæ²¡æœ‰åŸå§‹case_idsï¼Œä½¿ç”¨æ ¼å¼åŒ–åçš„case_ids
                    all_case_ids = suggestion["case_ids"]

                if all_case_ids:
                    # å°è¯•æŸ¥æ‰¾æ–°æ ¼å¼ID (å¦‚FT-xxx, ST-xxx)
                    new_format_ids = [cid for cid in all_case_ids if isinstance(cid, str) and
                                     (cid.startswith("FT-") or
                                      cid.startswith("ST-") or
                                      cid.startswith("CT-") or
                                      cid.startswith("PT-") or
                                      cid.startswith("BT-") or
                                      cid.startswith("ET-"))]

                    # å¦‚æœæ‰¾åˆ°æ–°æ ¼å¼IDï¼Œä½¿ç”¨å®ƒä½œä¸ºèŠ‚ç‚¹IDï¼›å¦åˆ™ä½¿ç”¨ç¬¬ä¸€ä¸ªID
                    node_id = new_format_ids[0] if new_format_ids else all_case_ids[0]

                    # ç”Ÿæˆè¦æ˜¾ç¤ºçš„case_idsæ–‡æœ¬
                    if isinstance(all_case_ids, list):
                        # æ˜¾ç¤ºåŸå§‹case_idsï¼Œä¸åšæ ¼å¼è½¬æ¢
                        display_ids = all_case_ids
                        case_ids = "/".join([str(cid) for cid in display_ids[:2]])
                        if len(display_ids) > 2:
                            case_ids += "..."
                    else:
                        case_ids = str(all_case_ids)
                else:
                    # å¦‚æœæ²¡æœ‰case_idsï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºèŠ‚ç‚¹ID
                    node_id = f"Case{index}"

                # æå–æ ‡é¢˜
                if "merged_case" in suggestion and "title" in suggestion["merged_case"]:
                    title = suggestion["merged_case"]["title"]
                elif "title" in suggestion:
                    title = suggestion["title"]
                else:
                    title = f"åˆå¹¶ç”¨ä¾‹ {index}"

            else:
                # å¦‚æœsuggestionä¸æ˜¯å­—å…¸ï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºèŠ‚ç‚¹ID
                node_id = f"Case{index}"

            # é˜²æ­¢æ ‡é¢˜è¿‡é•¿
            if len(title) > 30:
                title = title[:27] + "..."

            # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…Mermaidè¯­æ³•é”™è¯¯
            title = title.replace("(", "").replace(")", "").replace("[", "").replace("]", "")

            # ç¡®ä¿èŠ‚ç‚¹IDä¸å«ç‰¹æ®Šå­—ç¬¦
            node_id = ''.join(c for c in str(node_id) if c.isalnum() or c in ['-', '_'])

            # æ·»åŠ åˆ°å›¾è¡¨ä¸­
            merge_chart += f"    {node_id}[\"{case_ids}\"] --> Merge{index}[\"{title}\"]\n"

        merge_chart += "```\n\n"
    else:
        # æ²¡æœ‰åˆå¹¶å»ºè®®æˆ–åˆå¹¶å»ºè®®æ ¼å¼ä¸é€‚åˆç”Ÿæˆå›¾è¡¨
        merge_chart = "### ğŸ› ï¸ åˆå¹¶å»ºè®®æ–¹æ¡ˆ\n\n"
        merge_chart += "> å½“å‰æµ‹è¯•ç”¨ä¾‹ä¸éœ€è¦åˆå¹¶æˆ–æ²¡æœ‰æä¾›åˆå¹¶å»ºè®®ä¿¡æ¯\n\n"

    # å°†åˆå¹¶å»ºè®®å›¾æ·»åŠ åˆ°é‡å¤åˆ†æåé¢
    duplicate_combined_chart += merge_chart

    # æ·»åŠ æ ‘çŠ¶è¯„ä¼°æ¡†æ¶å›¾æ¨¡æ¿
    evaluation_framework_chart = """## ğŸŒ³ æµ‹è¯•ç”¨ä¾‹è¯„ä¼°æ¡†æ¶
```mermaid
graph TD
    A[æµ‹è¯•ç”¨ä¾‹è¯„ä¼°] --> B[æ ¼å¼åˆè§„æ€§]
    A --> C[å†…å®¹è´¨é‡]
    A --> D[åŠŸèƒ½è¦†ç›–]
    A --> E[å·¥ç¨‹æ•ˆç‡]
    A --> F[å®‰å…¨æ€§]

    C --> C1[å†…å®¹å‡†ç¡®æ€§]
    C --> C2[è¯­ä¹‰è´¨é‡]

    D --> D1[æµ‹è¯•è¦†ç›–åº¦]
    D --> D2[åŠŸèƒ½è¦†ç›–åº¦]
    D --> D3[ç¼ºé™·å‘ç°èƒ½åŠ›]

    E --> E1[é‡å¤æ€§åˆ†æ]
    E --> E2[å·¥ç¨‹æ•ˆç‡]

    F --> F1[å®‰å…¨ä¸ç»æµæ€§]

    classDef important fill:#f9d77e,stroke:#f9a11b,stroke-width:2px;
    classDef quality fill:#a8d6ff,stroke:#4a86e8,stroke-width:2px;
    classDef coverage fill:#b6d7a8,stroke:#6aa84f,stroke-width:2px;

    class A,D important;
    class B,C,F quality;
    class D1,D2,D3 coverage;
```

"""

    # æ£€æŸ¥æ˜¯å¦æ˜¯CollabEvalç»“æœ
    is_collab_eval = evaluation_result.get("collab_eval_result", False)
    # æ£€æŸ¥å®é™…ä½¿ç”¨çš„è¯„ä¼°æ¡†æ¶
    actual_framework = "Standard"
    if "evaluation_framework" in evaluation_result:
        actual_framework = evaluation_result["evaluation_framework"]
    elif "committee_summary" in evaluation_result and "evaluation_framework" in evaluation_result["committee_summary"]:
        actual_framework = evaluation_result["committee_summary"]["evaluation_framework"]
    elif "committee_info" in evaluation_result and "evaluation_framework" in evaluation_result["committee_info"]:
        actual_framework = evaluation_result["committee_info"]["evaluation_framework"]

    # æ ¹æ®å®é™…è¯„ä¼°æ¡†æ¶è®¾ç½®æ ‡å¿—
    is_using_collab_eval = actual_framework == "CollabEval"
    collab_eval_info = ""

    if is_collab_eval and "committee_summary" in evaluation_result:
        committee_summary = evaluation_result["committee_summary"]

        # æ ¹æ®å®é™…ä½¿ç”¨çš„æ¡†æ¶é€‰æ‹©ä¸åŒçš„å›¾è¡¨æ¨¡æ¿
        if is_using_collab_eval:
            collab_eval_info = """### ğŸ”„ è¯„æµ‹æµç¨‹æ¡†æ¶
```mermaid
graph TD
    A[CollabEvalè¯„æµ‹æ¡†æ¶] --> B[é˜¶æ®µ1: ç‹¬ç«‹è¯„åˆ†]
    A --> C[é˜¶æ®µ2: è¾©è®ºåä½œ]
    A --> D[é˜¶æ®µ3: ä¸»å¸­èšåˆ]
    
    B --> B1[è¯„å§”ä¸“å±Prompt]
    B --> B2[è®¡ç®—åˆå§‹å…±è¯†åº¦]
    
    C --> C1[ä½å…±è¯†è§¦å‘è¾©è®º]
    C --> C2[æ€ç»´æ ‘æ¡†æ¶]
    
    D --> D1[ä¸»å¸­åŠ æƒèšåˆ]
    D --> D2[æ ‡è®°é«˜äº‰è®®ç”¨ä¾‹]
    
    classDef highlight fill:#f9d77e,stroke:#f9a11b,stroke-width:2px;
    class A,C,D highlight;
```

"""
        else:
            collab_eval_info = """### ğŸ”„ æ ‡å‡†å¤šè¯„å§”è¯„æµ‹æ¡†æ¶
```mermaid
graph TD
    A[æ ‡å‡†å¤šè¯„å§”è¯„æµ‹] --> B[é˜¶æ®µ1: ç‹¬ç«‹è¯„åˆ†]
    A --> D[é˜¶æ®µ2: ç»“æœèšåˆ]
    
    B --> B1[è¯„å§”ç‹¬ç«‹è¯„ä¼°]
    B --> B2[åŸºäºä¸“ä¸šé¢†åŸŸè¯„åˆ†]
    
    D --> D1[åŠ æƒå¹³å‡è®¡ç®—]
    D --> D2[æœ€ç»ˆåˆ†æ•°ç¡®å®š]
    
    classDef highlight fill:#f9d77e,stroke:#f9a11b,stroke-width:2px;
    class A,D highlight;
```

"""

        # æ·»åŠ è¯„å§”è¯„åˆ†æƒ…å†µ
        if "judge_scores" in committee_summary:
            collab_eval_info += "### è¯„å§”è¯„åˆ†æƒ…å†µ\n"
            collab_eval_info += "| è¯„å§”æ¨¡å‹ | æ€»ä½“è¯„åˆ† |\n"
            collab_eval_info += "|---------|--------|\n"

            for judge, score in committee_summary["judge_scores"].items():
                collab_eval_info += f"| {judge} | {score} |\n"

            collab_eval_info += "\n"

        # æ·»åŠ é«˜äº‰è®®ç»´åº¦
        if "high_disagreement_dimensions" in committee_summary and committee_summary["high_disagreement_dimensions"]:
            high_disagreement = committee_summary["high_disagreement_dimensions"]
            collab_eval_info += "### é«˜äº‰è®®ç»´åº¦\n"
            for dimension in high_disagreement:
                collab_eval_info += f"- {dimension.replace('_', ' ').title()}\n"
            collab_eval_info += "\n"

        # æ·»åŠ ä¸»å¸­å†³ç­–ç†ç”±
        if is_using_collab_eval and "stage3_chairman_decision" in committee_summary:
            chairman_decision = committee_summary["stage3_chairman_decision"]
            if "chairman_decision" in chairman_decision and "rationale" in chairman_decision["chairman_decision"]:
                collab_eval_info += "### ä¸»å¸­å†³ç­–ç†ç”±\n"
                collab_eval_info += f"> {chairman_decision['chairman_decision']['rationale']}\n\n"

            # æ·»åŠ é«˜äº‰è®®åŒºåŸŸ
            if "chairman_decision" in chairman_decision and "high_disagreement_areas" in chairman_decision["chairman_decision"]:
                high_areas = chairman_decision["chairman_decision"]["high_disagreement_areas"]
                if high_areas:
                    collab_eval_info += "### ä¸»å¸­æ ‡è®°çš„é«˜äº‰è®®åŒºåŸŸ\n"
                    for i, area in enumerate(high_areas):
                        collab_eval_info += f"{i+1}. {area}\n"
                    collab_eval_info += "\n"

    # åœ¨æç¤ºä¸­è¯´æ˜ä½¿ç”¨æ›´æ–°çš„å›¾è¡¨è¯­æ³•
    prompt = f"""
# ä»»åŠ¡
åŸºäºæä¾›çš„æµ‹è¯•ç”¨ä¾‹è¯„ä¼°ç»“æœï¼Œç”Ÿæˆä¸€ä»½è¯¦ç»†çš„Markdownæ ¼å¼è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…å«Mermaidå›¾è¡¨å¯è§†åŒ–å…³é”®æ•°æ®ã€‚
"""

    # å¦‚æœæ˜¯è¿­ä»£å¯¹æ¯”ï¼Œå¢åŠ è¿­ä»£å¯¹æ¯”ä»»åŠ¡è¯´æ˜
    if is_iteration and formatted_prev_cases:
        prompt += f"""
# è¿­ä»£å¯¹æ¯”ä»»åŠ¡
æœ¬æ¬¡è¯„ä¼°åŒ…å«è¿­ä»£å‰åå¯¹æ¯”åˆ†æï¼Œéœ€è¦é‡ç‚¹å…³æ³¨æµ‹è¯•ç”¨ä¾‹åœ¨æœ¬æ¬¡è¿­ä»£ä¸­çš„è´¨é‡æ”¹è¿›æƒ…å†µï¼Œå¹¶æå‡ºé’ˆå¯¹æ€§å»ºè®®ã€‚
"""

    prompt += f"""
# è¯„ä¼°ç»“æœ
```json
{json.dumps(evaluation_result, ensure_ascii=False, indent=2)}
```

# æŠ¥å‘Šè¦æ±‚
è¯·ç”Ÿæˆä¸€ä»½ä¸“ä¸šã€è¯¦ç»†çš„Markdownæ ¼å¼è¯„ä¼°æŠ¥å‘Šï¼ŒåŒ…å«ä»¥ä¸‹å†…å®¹ï¼š

1. **æŠ¥å‘Šæ ‡é¢˜ä¸æ‘˜è¦**ï¼š{("ã€CollabEvalä¸‰é˜¶æ®µè¯„æµ‹ã€‘" if is_using_collab_eval else "ã€å¤šè¯„å§”ç»¼åˆè¯„æµ‹ã€‘") if evaluation_result.get("is_committee_result", False) else ""}ç®€è¦æ€»ç»“è¯„ä¼°ç»“æœ"""

    # å¦‚æœæ˜¯è¿­ä»£å¯¹æ¯”ï¼Œåœ¨æ ‡é¢˜ä¸­æ·»åŠ è¿­ä»£å¯¹æ¯”æ ‡è¯†
    if is_iteration:
        prompt = prompt.replace("ç®€è¦æ€»ç»“è¯„ä¼°ç»“æœ", "ã€è¿­ä»£å¯¹æ¯”åˆ†æã€‘ç®€è¦æ€»ç»“è¯„ä¼°ç»“æœ")

    prompt += f"""
2. **è¯„ä¼°æŒ‡æ ‡ä¸æ–¹æ³•**ï¼šè¯´æ˜ä½¿ç”¨çš„è¯„ä¼°æ ‡å‡†å’Œæ–¹æ³•ï¼Œå¹¶åŒ…å«è¯„ä¼°æ¡†æ¶å›¾
{evaluation_framework_chart}
3. **ç»¼åˆè¯„åˆ†**ï¼šä½¿ç”¨æä¾›çš„è¡¨æ ¼å’Œé¥¼å›¾å±•ç¤ºå„ç»´åº¦è¯„åˆ†
{radar_chart}"""

    # å¦‚æœå¯ç”¨è¿­ä»£å¯¹æ¯”ï¼Œåœ¨é€‚å½“ä½ç½®æ·»åŠ è¿­ä»£å¯¹æ¯”å›¾è¡¨
    if is_iteration and iteration_comparison_chart:
        prompt += f"""
4. **è¿­ä»£å‰åå¯¹æ¯”**ï¼šåˆ†æå½“å‰è¿­ä»£ä¸ä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹è´¨é‡å˜åŒ–
{iteration_comparison_chart}"""

        # è°ƒæ•´åç»­å†…å®¹çš„åºå·
        prompt += """
5. **è¯¦ç»†åˆ†æ**ï¼š
   - åŠŸèƒ½è¦†ç›–åº¦åˆ†æ
   - ç¼ºé™·å‘ç°èƒ½åŠ›åˆ†æ
   - å·¥ç¨‹æ•ˆç‡åˆ†æ
   - è¯­ä¹‰è´¨é‡åˆ†æ
   - å®‰å…¨ä¸ç»æµæ€§åˆ†æ
6. **é‡å¤æµ‹è¯•ç”¨ä¾‹åˆ†æ**ï¼š
   - é‡å¤æµ‹è¯•ç”¨ä¾‹æ¯”ç‡ä¸ç±»å‹çš„ç»¼åˆåˆ†æ
   - æµ‹è¯•ç”¨ä¾‹åˆå¹¶å»ºè®®
"""
    else:
        prompt += """
4. **è¯¦ç»†åˆ†æ**ï¼š
   - åŠŸèƒ½è¦†ç›–åº¦åˆ†æ
   - ç¼ºé™·å‘ç°èƒ½åŠ›åˆ†æ
   - å·¥ç¨‹æ•ˆç‡åˆ†æ
   - è¯­ä¹‰è´¨é‡åˆ†æ
   - å®‰å…¨ä¸ç»æµæ€§åˆ†æ
5. **é‡å¤æµ‹è¯•ç”¨ä¾‹åˆ†æ**ï¼š
   - é‡å¤æµ‹è¯•ç”¨ä¾‹æ¯”ç‡ä¸ç±»å‹çš„ç»¼åˆåˆ†æ
   - æµ‹è¯•ç”¨ä¾‹åˆå¹¶å»ºè®®
"""

    # æ·»åŠ é‡å¤æµ‹è¯•ç”¨ä¾‹åˆ†æå›¾è¡¨
    prompt += f"""
{duplicate_combined_chart}
"""

    # ç»§ç»­æ·»åŠ è¦†ç›–ç‡åˆ†æç­‰åç»­å†…å®¹
    if is_iteration:
        prompt += f"""
7. **æµ‹è¯•è¦†ç›–ç‡åˆ†æ**ï¼š
   - å…³é”®æµç¨‹å’ŒåŠŸèƒ½è¦†ç›–æƒ…å†µ
{coverage_chart}
"""
    else:
        prompt += f"""
6. **æµ‹è¯•è¦†ç›–ç‡åˆ†æ**ï¼š
   - å…³é”®æµç¨‹å’ŒåŠŸèƒ½è¦†ç›–æƒ…å†µ
{coverage_chart}
"""

    # å¦‚æœæ˜¯CollabEvalç»“æœï¼Œæ·»åŠ CollabEvalä¿¡æ¯éƒ¨åˆ†
    if evaluation_result.get("collab_eval_result", False):
        if is_iteration:
            prompt += f"""
8. **{("CollabEvalä¸‰é˜¶æ®µè¯„æµ‹" if is_using_collab_eval else "æ ‡å‡†å¤šè¯„å§”è¯„æµ‹")}**ï¼š
   - {("ä¸‰é˜¶æ®µè¯„æµ‹æµç¨‹" if is_using_collab_eval else "è¯„æµ‹æµç¨‹")}
   - è¯„å§”è¯„åˆ†æƒ…å†µä¸äº‰è®®ç»´åº¦
   - {("ä¸»å¸­å†³ç­–ç†ç”±" if is_using_collab_eval else "ç»“æœèšåˆæœºåˆ¶")}
{collab_eval_info}
"""
        else:
            prompt += f"""
7. **{("CollabEvalä¸‰é˜¶æ®µè¯„æµ‹" if is_using_collab_eval else "æ ‡å‡†å¤šè¯„å§”è¯„æµ‹")}**ï¼š
   - {("ä¸‰é˜¶æ®µè¯„æµ‹æµç¨‹" if is_using_collab_eval else "è¯„æµ‹æµç¨‹")}
   - è¯„å§”è¯„åˆ†æƒ…å†µä¸äº‰è®®ç»´åº¦
   - {("ä¸»å¸­å†³ç­–ç†ç”±" if is_using_collab_eval else "ç»“æœèšåˆæœºåˆ¶")}
{collab_eval_info}
"""
    else:
        if is_iteration:
            prompt += """
8. **ä¼˜ç¼ºç‚¹å¯¹æ¯”**ï¼šåˆ—å‡ºAIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ç›¸å¯¹äºäººå·¥æ ‡å‡†çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿
"""
        else:
            prompt += """
7. **ä¼˜ç¼ºç‚¹å¯¹æ¯”**ï¼šåˆ—å‡ºAIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹ç›¸å¯¹äºäººå·¥æ ‡å‡†çš„ä¼˜åŠ¿å’ŒåŠ£åŠ¿
"""

    # æ·»åŠ é€šç”¨éƒ¨åˆ†ï¼Œæ ¹æ®æ˜¯å¦å¯ç”¨è¿­ä»£å¯¹æ¯”è°ƒæ•´åºå·
    if is_iteration:
        prompt += """
9. **æ”¹è¿›å»ºè®®**ï¼šç»™å‡º3-5æ¡å…·ä½“å¯è¡Œçš„æ”¹è¿›AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„å»ºè®®ï¼ŒåŒ…æ‹¬å¦‚ä½•å‡å°‘é‡å¤
10. **ç»¼åˆç»“è®º**ï¼šæ€»ç»“AIæµ‹è¯•ç”¨ä¾‹çš„æ•´ä½“è¡¨ç°å’Œé€‚ç”¨åœºæ™¯
"""
    else:
        prompt += """
8. **æ”¹è¿›å»ºè®®**ï¼šç»™å‡º3-5æ¡å…·ä½“å¯è¡Œçš„æ”¹è¿›AIç”Ÿæˆæµ‹è¯•ç”¨ä¾‹çš„å»ºè®®ï¼ŒåŒ…æ‹¬å¦‚ä½•å‡å°‘é‡å¤
9. **ç»¼åˆç»“è®º**ï¼šæ€»ç»“AIæµ‹è¯•ç”¨ä¾‹çš„æ•´ä½“è¡¨ç°å’Œé€‚ç”¨åœºæ™¯
"""

    # ç¾åŒ–è¦æ±‚
    prompt += """
# ç¾åŒ–è¦æ±‚
1. è¯·ä½¿ç”¨æ›´ä¸°å¯Œçš„Markdownæ ¼å¼å…ƒç´ æ¥å¢å¼ºæŠ¥å‘Šçš„å¯è¯»æ€§ï¼Œå¦‚é€‚å½“ä½¿ç”¨åˆ†éš”çº¿ã€å¼•ç”¨å—ã€è¡¨æƒ…ç¬¦å·ç­‰
2. ä¸ºå…³é”®æ•°æ®æ·»åŠ é†’ç›®çš„æ ‡è®°ï¼Œå¦‚é‡è¦çš„è¯„åˆ†ã€æ˜¾è‘—çš„å·®å¼‚ç­‰
3. åœ¨è¯„åˆ†éƒ¨åˆ†ä½¿ç”¨ä¸­æ–‡ç»´åº¦åç§°å’Œæ˜Ÿå·è¯„åˆ†å¯è§†åŒ–
4. ä¸ºæŠ¥å‘Šæ·»åŠ ç®€æ´ç¾è§‚çš„é¡µçœ‰é¡µè„š
5. æ·»åŠ æœ‰é’ˆå¯¹æ€§çš„æ”¹è¿›å»ºè®®ï¼Œä½¿ç»“è®ºæ›´å…·æ“ä½œæ€§

# ç‰¹åˆ«è¯´æ˜
- è¯·ä¸è¦åœ¨æŠ¥å‘Šä¸­æ·»åŠ "æ¨¡å—åˆ†å¸ƒ"ç›¸å…³çš„é¥¼å›¾ï¼Œåªä¿ç•™æ–‡å­—æè¿°å½¢å¼çš„æ¨¡å—åˆ†å¸ƒä¿¡æ¯
- ä¸è¦ä½¿ç”¨é¥¼å›¾å±•ç¤ºæµ‹è¯•ç”¨ä¾‹ç±»å‹åˆ†å¸ƒ
"""

    # å¦‚æœæ˜¯CollabEvalç»“æœï¼Œæ·»åŠ é¢å¤–è¯´æ˜
    if is_collab_eval:
        prompt += """
- åœ¨CollabEvaléƒ¨åˆ†ï¼Œçªå‡ºæ˜¾ç¤ºä¸‰é˜¶æ®µè¯„æµ‹çš„è¿‡ç¨‹å’Œä»·å€¼ï¼Œç‰¹åˆ«å¼ºè°ƒè¾©è®ºåä½œå¦‚ä½•æé«˜è¯„æµ‹è´¨é‡
- åœ¨æè¿°è¯„å§”è¯„åˆ†æ—¶ï¼Œå…³æ³¨åˆ†æ­§ç‚¹ä»¥åŠä¸»å¸­æ˜¯å¦‚ä½•å¤„ç†è¿™äº›åˆ†æ­§çš„
"""

    prompt += """
# é¡µè„šæ ¼å¼
è¯·åœ¨æŠ¥å‘Šæœ«å°¾æ·»åŠ ä¸€è¡Œé¡µè„šï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼ˆä¸è¦æ›¿æ¢DATETIME_PLACEHOLDERï¼Œä¿æŒåŸæ ·ï¼‰ï¼š
**ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**

ç³»ç»Ÿä¼šè‡ªåŠ¨å°†DATETIME_PLACEHOLDERæ›¿æ¢ä¸ºå®æ—¶æ—¶é—´ï¼Œè¯·ä¸è¦è‡ªè¡Œå¡«å†™ä»»ä½•æ—¥æœŸæ—¶é—´ã€‚

è¯·ç¡®ä¿ä½¿ç”¨ä¸Šé¢æä¾›çš„å›¾è¡¨æ¨¡æ¿ï¼Œè¿™äº›æ¨¡æ¿å·²ç»åŒ…å«äº†ä»è¯„ä¼°ç»“æœä¸­æå–çš„å®é™…æ•°æ®ã€‚
è¿™äº›å›¾è¡¨ä½¿ç”¨çš„æ˜¯è¾ƒä¸ºé€šç”¨çš„Mermaidè¯­æ³•ï¼Œç¡®ä¿ä¸å¤§å¤šæ•°MarkdownæŸ¥çœ‹å™¨å…¼å®¹ã€‚
ä½ å¯ä»¥æ ¹æ®è¯„ä¼°ç»“æœè°ƒæ•´å›¾è¡¨å†…å®¹ï¼Œä½†è¦ä¿æŒ```mermaidè¯­æ³•æ ¼å¼ã€‚
ç›´æ¥ä»¥# å¼€å¤´çš„æ ‡é¢˜å¼€å§‹ä½ çš„æŠ¥å‘Šï¼Œä¸è¦åœ¨å¼€å¤´å†™"markdown"ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
"""

    system_prompt = "ä½ æ˜¯ä¸€ä½ç²¾é€šè½¯ä»¶æµ‹è¯•å’ŒæŠ€æœ¯æ–‡æ¡£å†™ä½œçš„ä¸“å®¶ã€‚è¯·æ ¹æ®è¯„ä¼°ç»“æœç”Ÿæˆä¸€ä»½ä¸“ä¸šã€æ¸…æ™°çš„Markdownæ ¼å¼æŠ¥å‘Šï¼Œå¹¶ä½¿ç”¨Mermaidå›¾è¡¨å¯è§†åŒ–å…³é”®æ•°æ®ã€‚è¯·ç›´æ¥ä¿ç•™å¹¶ä½¿ç”¨æˆ‘æä¾›çš„è¯„åˆ†è¡¨æ ¼æ ¼å¼ï¼Œä¸è¦ä¿®æ”¹å…¶ç»“æ„ã€‚è¯·ç›´æ¥è¾“å‡ºMarkdownæ ¼å¼ï¼Œä¸è¦å°è¯•è¾“å‡ºJSONã€‚ä¸¥æ ¼ç¦æ­¢åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ 'markdown'è¿™ä¸ªè¯ï¼Œç›´æ¥ä»¥'# 'å¼€å¤´çš„æ ‡é¢˜å¼€å§‹ã€‚ä¸è¦åœ¨å†…å®¹å¤–åŒ…å«```æˆ–```markdownæ ‡è®°ï¼Œå®Œå…¨é¿å…ä½¿ç”¨ä»£ç å—ï¼Œä½†ä¿ç•™æä¾›çš„Mermaidå›¾è¡¨è¯­æ³•ã€‚"

    # ä½¿ç”¨è¾ƒé«˜çš„temperatureå€¼ï¼Œç”Ÿæˆæ›´æœ‰åˆ›æ„çš„æŠ¥å‘Š
    max_retries = 3
    for retry_count in range(max_retries):
        try:
            # æ·»åŠ éšæœºè¯·æ±‚IDï¼Œç¡®ä¿æ¯æ¬¡è¯·æ±‚éƒ½æ˜¯å…¨æ–°çš„
            request_id = f"report-{int(time.time())}-{retry_count}-{uuid.uuid4().hex[:8]}"
            log(f"ç”ŸæˆæŠ¥å‘Šè¯·æ±‚ID: {request_id}")
            
            result = await async_call_llm(
                session,
                prompt,
                system_prompt,
                temperature=LLM_TEMPERATURE_REPORT,  # ä½¿ç”¨é…ç½®ä¸­çš„è¾ƒé«˜temperatureå€¼
                use_cache=False,  # ç¦ç”¨ç¼“å­˜ï¼Œç¡®ä¿æ¯æ¬¡è¯„æµ‹éƒ½æ˜¯å…¨æ–°çš„
                retries=2  # è®¾ç½®å†…éƒ¨é‡è¯•æ¬¡æ•°
            )

            if not result:
                log_error(f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥ (å°è¯• {retry_count + 1}/{max_retries})", important=True)
                if retry_count < max_retries - 1:
                    log(f"ç­‰å¾…2ç§’åé‡è¯•...", important=True)
                    await asyncio.sleep(2)
                    continue
                else:
                    return "# è¯„æµ‹æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\næ— æ³•ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼Œè¯·æ£€æŸ¥è¯„æµ‹ç»“æœæˆ–é‡è¯•ã€‚"

            # æ£€æŸ¥è¿”å›çš„ç»“æœç±»å‹
            if isinstance(result, dict):
                # å¦‚æœè¿”å›çš„æ˜¯å­—å…¸ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«æ–‡æœ¬å†…å®¹
                if "text" in result and result["text"]:
                    # è¿”å›æ–‡æœ¬å†…å®¹
                    markdown_content = result["text"]
                    log("æˆåŠŸç”ŸæˆMarkdownæŠ¥å‘Š", important=True)
                    # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
                    await asyncio.sleep(0.1)

                    # åœ¨è¿”å›å‰æ›¿æ¢å ä½ç¬¦
                    from datetime import datetime
                    current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
                    footer = f"**ç”Ÿæˆæ—¶é—´ï¼š{current_time} â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**"

                    # æ›¿æ¢å ä½ç¬¦
                    import re
                    placeholder_patterns = [
                        r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                        r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                        r"\*\*ç”Ÿæˆæ—¶é—´ï¼š<.*?>(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
                    ]

                    placeholder_found = False
                    for pattern in placeholder_patterns:
                        if re.search(pattern, markdown_content):
                            markdown_content = re.sub(pattern, footer, markdown_content)
                            placeholder_found = True
                            log("å·²æ›¿æ¢markdown_contentä¸­çš„å ä½ç¬¦ä¸ºå®æ—¶æ—¶é—´", important=True)
                            break

                    if not placeholder_found:
                        log("markdown_contentä¸­æœªæ‰¾åˆ°å ä½ç¬¦ï¼Œå°†æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ ¼å¼çš„æ—¶é—´æˆ³", level="WARNING")

                    return markdown_content
                elif "error" in result:
                    # è¿”å›é”™è¯¯ä¿¡æ¯
                    log_error(f"ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {result['error']}")
                    if retry_count < max_retries - 1:
                        log(f"ç­‰å¾…2ç§’åé‡è¯•...", important=True)
                        await asyncio.sleep(2)
                        continue
                    else:
                        return f"# è¯„æµ‹æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\n{result['error']}"
                elif "api_response" in result:
                    # APIè¿”å›äº†åŸå§‹å“åº”ä½†æ— æ³•è§£æ
                    log_error(f"APIè¿”å›çš„å“åº”æ— æ³•è§£æä¸ºæœ‰æ•ˆå†…å®¹ (å°è¯• {retry_count + 1}/{max_retries})")
                    if retry_count < max_retries - 1:
                        log(f"ç­‰å¾…2ç§’åé‡è¯•...", important=True)
                        await asyncio.sleep(2)
                        continue
                    else:
                        # å°è¯•ç”Ÿæˆä¸€ä¸ªåŸºæœ¬æŠ¥å‘Š
                        return generate_basic_report(evaluation_result)
                else:
                    # å°è¯•å°†å­—å…¸è½¬æ¢ä¸ºMarkdown
                    try:
                        md_content = "# AIæµ‹è¯•ç”¨ä¾‹è¯„ä¼°æŠ¥å‘Š\n\n"

                        if "evaluation_summary" in result:
                            summary = result["evaluation_summary"]
                            md_content += f"## æ‘˜è¦\n\n"
                            md_content += f"**æ€»ä½“è¯„åˆ†**: {summary.get('overall_score', 'N/A')}\n\n"
                            md_content += f"**æ”¹è¿›å»ºè®®**: {summary.get('final_suggestion', 'N/A')}\n\n"

                        if "detailed_report" in result:
                            md_content += f"## è¯¦ç»†è¯„ä¼°\n\n"
                            detailed = result["detailed_report"]

                            for key, value in detailed.items():
                                if isinstance(value, dict) and "score" in value:
                                    md_content += f"### {key.replace('_', ' ').title()}\n\n"
                                    md_content += f"**è¯„åˆ†**: {value.get('score', 'N/A')}\n\n"
                                    md_content += f"**ç†ç”±**: {value.get('reason', 'N/A')}\n\n"

                                    if key == "duplicate_analysis" and "merge_suggestions" in value:
                                        md_content += f"**åˆå¹¶å»ºè®®**: {value.get('merge_suggestions', 'N/A')}\n\n"

                                    if "analysis" in value and isinstance(value["analysis"], dict):
                                        analysis = value["analysis"]
                                        if "covered_features" in analysis:
                                            md_content += "**è¦†ç›–çš„åŠŸèƒ½**:\n\n"
                                            for feature in analysis["covered_features"]:
                                                md_content += f"- {feature}\n"
                                            md_content += "\n"

                                        if "missed_features_or_scenarios" in analysis:
                                            md_content += "**æœªè¦†ç›–çš„åŠŸèƒ½æˆ–åœºæ™¯**:\n\n"
                                            for feature in analysis["missed_features_or_scenarios"]:
                                                md_content += f"- {feature}\n"
                                            md_content += "\n"

                                        if "scenario_types_found" in analysis:
                                            md_content += "**å‘ç°çš„åœºæ™¯ç±»å‹**:\n\n"
                                            for scenario in analysis["scenario_types_found"]:
                                                md_content += f"- {scenario}\n"
                                            md_content += "\n"

                        log("æˆåŠŸä»å­—å…¸ç”ŸæˆMarkdownæŠ¥å‘Š", important=True)
                        # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
                        await asyncio.sleep(0.1)

                        # åœ¨è¿”å›å‰æ›¿æ¢å ä½ç¬¦
                        from datetime import datetime
                        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
                        footer = f"**ç”Ÿæˆæ—¶é—´ï¼š{current_time} â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**"

                        # æ›¿æ¢å ä½ç¬¦
                        import re
                        placeholder_patterns = [
                            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                            r"\*\*ç”Ÿæˆæ—¶é—´ï¼š<.*?>(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
                        ]

                        placeholder_found = False
                        for pattern in placeholder_patterns:
                            if re.search(pattern, md_content):
                                md_content = re.sub(pattern, footer, md_content)
                                placeholder_found = True
                                log("å·²æ›¿æ¢md_contentä¸­çš„å ä½ç¬¦ä¸ºå®æ—¶æ—¶é—´", important=True)
                                break

                        if not placeholder_found:
                            log("md_contentä¸­æœªæ‰¾åˆ°å ä½ç¬¦ï¼Œå°†æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ ¼å¼çš„æ—¶é—´æˆ³", level="WARNING")

                        return md_content
                    except Exception as e:
                        log_error(f"ä»å­—å…¸ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}")
                        if retry_count < max_retries - 1:
                            log(f"ç­‰å¾…2ç§’åé‡è¯•...", important=True)
                            await asyncio.sleep(2)
                            continue
                        else:
                            # å¦‚æœæ— æ³•è½¬æ¢ä¸ºMarkdownï¼Œç›´æ¥è¿”å›JSONå­—ç¬¦ä¸²
                            return f"# è¯„æµ‹æŠ¥å‘Š\n\n```\n{json.dumps(result, ensure_ascii=False, indent=2)}\n```"

            # å¦‚æœè¿”å›çš„ä¸æ˜¯å­—å…¸ï¼Œè€Œæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
            if isinstance(result, str):
                log("LLMç›´æ¥è¿”å›äº†Markdownæ–‡æœ¬", important=True)
                # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
                await asyncio.sleep(0.1)
                # åˆ é™¤Markdownæ–‡æœ¬é¡¶éƒ¨çš„"markdown"å‰ç¼€
                if result.strip().startswith("markdown"):
                    result = result.strip().replace("markdown", "", 1).strip()
                    log("å·²åˆ é™¤Markdownæ–‡æœ¬é¡¶éƒ¨çš„'markdown'å‰ç¼€", important=True)
                    # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
                    await asyncio.sleep(0.1)

                # åœ¨è¿”å›å‰æ›¿æ¢å ä½ç¬¦
                from datetime import datetime
                current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
                footer = f"**ç”Ÿæˆæ—¶é—´ï¼š{current_time} â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**"

                # æ›¿æ¢å ä½ç¬¦
                import re
                placeholder_patterns = [
                    r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                    r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                    r"\*\*ç”Ÿæˆæ—¶é—´ï¼š<.*?>(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
                ]

                placeholder_found = False
                for pattern in placeholder_patterns:
                    if re.search(pattern, result):
                        result = re.sub(pattern, footer, result)
                        placeholder_found = True
                        log("å·²æ›¿æ¢LLMç”ŸæˆæŠ¥å‘Šä¸­çš„å ä½ç¬¦ä¸ºå®æ—¶æ—¶é—´", important=True)
                        break

                if not placeholder_found:
                    log("LLMç”ŸæˆæŠ¥å‘Šä¸­æœªæ‰¾åˆ°å ä½ç¬¦ï¼Œå°†æ£€æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ ¼å¼çš„æ—¶é—´æˆ³", level="WARNING")

                return result

            # å°è¯•ä½¿ç”¨extract_valid_jsonå‡½æ•°ä»ç»“æœä¸­æå–æœ‰æ•ˆçš„JSON
            try:
                log("å°è¯•ä»LLMå“åº”ä¸­æå–æœ‰æ•ˆçš„JSON", important=True)
                extracted_json = extract_valid_json(str(result))
                if extracted_json:
                    log("æˆåŠŸä»LLMå“åº”ä¸­æå–æœ‰æ•ˆçš„JSON", important=True)

                    # å°†æå–çš„JSONè½¬æ¢ä¸ºMarkdown
                    try:
                        md_content = "# AIæµ‹è¯•ç”¨ä¾‹è¯„ä¼°æŠ¥å‘Š\n\n"

                        if "evaluation_summary" in extracted_json:
                            summary = extracted_json["evaluation_summary"]
                            md_content += f"## æ‘˜è¦\n\n"
                            md_content += f"**æ€»ä½“è¯„åˆ†**: {summary.get('overall_score', 'N/A')}\n\n"
                            md_content += f"**æ”¹è¿›å»ºè®®**: {summary.get('final_suggestion', 'N/A')}\n\n"

                        if "detailed_report" in extracted_json:
                            md_content += f"## è¯¦ç»†è¯„ä¼°\n\n"
                            detailed = extracted_json["detailed_report"]

                            for key, value in detailed.items():
                                if isinstance(value, dict) and "score" in value:
                                    md_content += f"### {key.replace('_', ' ').title()}\n\n"
                                    md_content += f"**è¯„åˆ†**: {value.get('score', 'N/A')}\n\n"
                                    md_content += f"**ç†ç”±**: {value.get('reason', 'N/A')}\n\n"

                                    if key == "duplicate_analysis" and "merge_suggestions" in value:
                                        md_content += f"**åˆå¹¶å»ºè®®**: {value.get('merge_suggestions', 'N/A')}\n\n"

                                    if "analysis" in value and isinstance(value["analysis"], dict):
                                        analysis = value["analysis"]
                                        if "covered_features" in analysis:
                                            md_content += "**è¦†ç›–çš„åŠŸèƒ½**:\n\n"
                                            for feature in analysis["covered_features"]:
                                                md_content += f"- {feature}\n"
                                            md_content += "\n"

                                        if "missed_features_or_scenarios" in analysis:
                                            md_content += "**æœªè¦†ç›–çš„åŠŸèƒ½æˆ–åœºæ™¯**:\n\n"
                                            for feature in analysis["missed_features_or_scenarios"]:
                                                md_content += f"- {feature}\n"
                                            md_content += "\n"

                                        if "scenario_types_found" in analysis:
                                            md_content += "**å‘ç°çš„åœºæ™¯ç±»å‹**:\n\n"
                                            for scenario in analysis["scenario_types_found"]:
                                                md_content += f"- {scenario}\n"
                                            md_content += "\n"

                        log("æˆåŠŸä»æå–çš„JSONç”ŸæˆMarkdownæŠ¥å‘Š", important=True)

                        # åœ¨è¿”å›å‰æ›¿æ¢å ä½ç¬¦
                        from datetime import datetime
                        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
                        footer = f"**ç”Ÿæˆæ—¶é—´ï¼š{current_time} â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**"

                        # æ›¿æ¢å ä½ç¬¦
                        import re
                        placeholder_patterns = [
                            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
                            r"\*\*ç”Ÿæˆæ—¶é—´ï¼š<.*?>(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
                        ]

                        placeholder_found = False
                        for pattern in placeholder_patterns:
                            if re.search(pattern, md_content):
                                md_content = re.sub(pattern, footer, md_content)
                                placeholder_found = True
                                log("å·²æ›¿æ¢æå–JSONç”Ÿæˆçš„md_contentä¸­çš„å ä½ç¬¦ä¸ºå®æ—¶æ—¶é—´", important=True)
                                break

                        if not placeholder_found:
                            log("æå–JSONç”Ÿæˆçš„md_contentä¸­æœªæ‰¾åˆ°å ä½ç¬¦", level="WARNING")

                        return md_content
                    except Exception as e:
                        log_error(f"ä»æå–çš„JSONç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥: {e}")
                else:
                    log_error("ä»LLMå“åº”ä¸­æå–æœ‰æ•ˆçš„JSONå¤±è´¥")
            except Exception as e:
                log_error(f"å°è¯•æå–JSONæ—¶å‡ºé”™: {e}")

            if retry_count < max_retries - 1:
                log(f"ç­‰å¾…2ç§’åé‡è¯•...", important=True)
                await asyncio.sleep(2)
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œç”ŸæˆåŸºæœ¬æŠ¥å‘Š
                return generate_basic_report(evaluation_result)
        except Exception as e:
            log_error(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‘ç”Ÿå¼‚å¸¸ (å°è¯• {retry_count + 1}/{max_retries}): {str(e)}")
            if retry_count < max_retries - 1:
                log(f"ç­‰å¾…2ç§’åé‡è¯•...", important=True)
                await asyncio.sleep(2)
            else:
                # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥ï¼Œç”ŸæˆåŸºæœ¬æŠ¥å‘Š
                return generate_basic_report(evaluation_result)

    # å¦‚æœæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªåŸºæœ¬æŠ¥å‘Š
    return generate_basic_report(evaluation_result)


def generate_basic_report(evaluation_result):
    """
    å½“LLMç”ŸæˆæŠ¥å‘Šå¤±è´¥æ—¶ï¼Œç”Ÿæˆä¸€ä¸ªåŸºæœ¬çš„æŠ¥å‘Š
    
    :param evaluation_result: è¯„æµ‹ç»“æœ
    :return: åŸºæœ¬çš„MarkdownæŠ¥å‘Š
    """
    try:
        # æå–è¯„åˆ†
        overall_score = "N/A"
        if isinstance(evaluation_result, dict) and "evaluation_summary" in evaluation_result:
            overall_score = evaluation_result["evaluation_summary"].get("overall_score", "N/A")
        
        # æå–å»ºè®®
        final_suggestion = "æ— æ³•è·å–å»ºè®®"
        if isinstance(evaluation_result, dict) and "evaluation_summary" in evaluation_result:
            final_suggestion = evaluation_result["evaluation_summary"].get("final_suggestion", "æ— æ³•è·å–å»ºè®®")
        
        # æ„å»ºåŸºæœ¬æŠ¥å‘Š
        report = f"""# AIæµ‹è¯•ç”¨ä¾‹è¯„ä¼°æŠ¥å‘Š

## æ‘˜è¦

**æ€»ä½“è¯„åˆ†**: {overall_score}/5.0

**æ”¹è¿›å»ºè®®**: {final_suggestion}

## è¯¦ç»†è¯„ä¼°

"""
        
        # æ·»åŠ è¯¦ç»†è¯„ä¼°
        if isinstance(evaluation_result, dict) and "detailed_report" in evaluation_result:
            detailed = evaluation_result["detailed_report"]
            for key, value in detailed.items():
                if isinstance(value, dict) and "score" in value:
                    report += f"### {key.replace('_', ' ').title()}\n\n"
                    report += f"**è¯„åˆ†**: {value.get('score', 'N/A')}\n\n"
                    report += f"**ç†ç”±**: {value.get('reason', 'N/A')}\n\n"
        
        # æ·»åŠ é¡µè„š
        from datetime import datetime
        # ç¡®ä¿ä½¿ç”¨å®æ—¶æ—¶é—´
        current_time = datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")

        # æ„å»ºé¡µè„š
        footer = f"**ç”Ÿæˆæ—¶é—´ï¼š{current_time} â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ**"
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰é¡µè„šï¼Œå¦‚æœæœ‰åˆ™æ›¿æ¢ï¼Œå¦åˆ™æ·»åŠ 
        import re
        footer_pattern = r"\*\*ç”Ÿæˆæ—¶é—´ï¼š(.*?)(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
        placeholder_patterns = [
            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER â€¢ gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
            r"\*\*ç”Ÿæˆæ—¶é—´ï¼šDATETIME_PLACEHOLDER(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*",
            r"\*\*ç”Ÿæˆæ—¶é—´ï¼š<.*?>(?:â€¢|Â·|\*) *gogogoå‡ºå‘å–½è¯„ä¼°ä¸­å¿ƒ\*\*"
        ]
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ˜ç¡®çš„å ä½ç¬¦
        placeholder_found = False
        for pattern in placeholder_patterns:
            if re.search(pattern, report):
                report = re.sub(pattern, footer, report)
                placeholder_found = True
                log("å·²æ›¿æ¢åŸºæœ¬æŠ¥å‘Šé¡µè„šä¸­çš„æ˜ç¡®å ä½ç¬¦ä¸ºå®æ—¶æ—¶é—´", important=True)
                break
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ç¡®çš„å ä½ç¬¦ï¼Œå°è¯•ä½¿ç”¨é€šç”¨æ¨¡å¼
        if not placeholder_found and re.search(footer_pattern, report):
            report = re.sub(footer_pattern, footer, report)
            log("å·²æ›¿æ¢åŸºæœ¬æŠ¥å‘Šé¡µè„šä¸­çš„æ—¥æœŸä¸ºå®æ—¶æ—¶é—´", important=True)
        elif not placeholder_found:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°é¡µè„šï¼Œåˆ™æ·»åŠ åˆ°æŠ¥å‘Šæœ«å°¾
            report += f"\n\n---\n{footer}\n"
            log("æœªæ‰¾åˆ°é¡µè„šï¼Œå·²æ·»åŠ å¸¦æœ‰å®æ—¶æ—¶é—´çš„é¡µè„šåˆ°åŸºæœ¬æŠ¥å‘Š", important=True)
        
        log("æˆåŠŸç”ŸæˆåŸºæœ¬MarkdownæŠ¥å‘Š", important=True)
        return report
        
    except Exception as e:
        log_error(f"ç”ŸæˆåŸºæœ¬æŠ¥å‘Šå¤±è´¥: {str(e)}")
        return "# è¯„æµ‹æŠ¥å‘Šç”Ÿæˆå¤±è´¥\n\næ— æ³•ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼Œè¯·æ£€æŸ¥è¯„æµ‹ç»“æœæˆ–é‡è¯•ã€‚"


async def evaluate_and_generate_report(session: aiohttp.ClientSession, ai_cases, golden_cases, report_file, is_iteration=False, prev_iteration_cases=None, evaluation_result=None):
    """
    ç”ŸæˆMarkdownæŠ¥å‘Š

    :param session: aiohttpä¼šè¯
    :param ai_cases: AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
    :param golden_cases: é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
    :param report_file: æŠ¥å‘Šæ–‡ä»¶è·¯å¾„
    :param is_iteration: æ˜¯å¦å¯ç”¨è¿­ä»£å‰åå¯¹æ¯”åŠŸèƒ½
    :param prev_iteration_cases: ä¸Šä¸€æ¬¡è¿­ä»£çš„æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯é€‰ï¼‰ï¼Œä»…åœ¨is_iterationä¸ºtrueæ—¶æœ‰æ•ˆ
    :param evaluation_result: å·²æœ‰çš„è¯„æµ‹ç»“æœï¼ˆå¯é€‰ï¼‰ï¼Œå¦‚æœæä¾›åˆ™ä¸å†è¿›è¡Œè¯„æµ‹
    :return: è¯„ä¼°ç»“æœå’ŒMarkdownæŠ¥å‘Š
    """
    log("å¼€å§‹ç”ŸæˆæŠ¥å‘Š", important=True)

    # å¦‚æœæ²¡æœ‰æä¾›è¯„æµ‹ç»“æœï¼Œåˆ™è¿›è¡Œè¯„æµ‹
    if evaluation_result is None:
        log("æœªæä¾›è¯„æµ‹ç»“æœï¼Œå¼€å§‹è¿›è¡Œè¯„æµ‹", important=True)
        evaluation_result = await evaluate_test_cases(session, ai_cases, golden_cases, is_iteration, prev_iteration_cases)

        if not evaluation_result:
            log("æµ‹è¯•ç”¨ä¾‹è¯„æµ‹å¤±è´¥", important=True)
            return {
                "success": False,
                "error": "æµ‹è¯•ç”¨ä¾‹è¯„æµ‹å¤±è´¥"
            }
    else:
        log("ä½¿ç”¨å·²æœ‰è¯„æµ‹ç»“æœç”ŸæˆæŠ¥å‘Š", important=True)

    # ç”ŸæˆæŠ¥å‘Š
    markdown_report = None
    markdown_report_iteration = None
    
    # å¦‚æœæ˜¯è¿­ä»£æ¨¡å¼ï¼Œéœ€è¦ç”Ÿæˆä¸¤ç§æŠ¥å‘Šï¼šç®€æ´çš„è¿­ä»£æŠ¥å‘Šå’Œå®Œæ•´çš„æ ‡å‡†æŠ¥å‘Š
    if is_iteration:
        # ç”Ÿæˆç®€æ´çš„è¿­ä»£æŠ¥å‘Š
        log("ç”Ÿæˆç®€æ´çš„è¿­ä»£æŠ¥å‘Š", important=True)
        markdown_report_iteration = await generate_markdown_report(session, evaluation_result, is_iteration=True, 
                                                           formatted_ai_cases=ai_cases, formatted_prev_cases=prev_iteration_cases)
        
        # ç”Ÿæˆå®Œæ•´çš„æ ‡å‡†æŠ¥å‘Šï¼ˆä¸ä½¿ç”¨è¿­ä»£å‚æ•°ï¼‰
        log("ç”Ÿæˆå®Œæ•´çš„æ ‡å‡†æŠ¥å‘Š", important=True)
        markdown_report = await generate_markdown_report(session, evaluation_result, is_iteration=False, 
                                                 formatted_ai_cases=ai_cases)
    else:
        # éè¿­ä»£æ¨¡å¼ï¼Œåªç”Ÿæˆä¸€ç§æŠ¥å‘Š
        markdown_report = await generate_markdown_report(session, evaluation_result, 
                                                 formatted_ai_cases=ai_cases)

    # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªæŠ¥å‘Šç”ŸæˆæˆåŠŸ
    if (not markdown_report and not markdown_report_iteration) or (markdown_report and len(markdown_report.strip()) < 10):
        # å¦‚æœæŠ¥å‘Šä¸ºç©ºæˆ–å†…å®¹å¤ªå°‘ï¼Œç”Ÿæˆä¸€ä¸ªåŸºæœ¬æŠ¥å‘Š
        log("ç”Ÿæˆçš„MarkdownæŠ¥å‘Šä¸ºç©ºæˆ–å†…å®¹ä¸è¶³ï¼Œå°è¯•ç”ŸæˆåŸºæœ¬æŠ¥å‘Š", important=True)
        markdown_report = generate_basic_report(evaluation_result)
        
        if not markdown_report or len(markdown_report.strip()) < 10:
            log("ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥", important=True)
            return {
                "success": False,
                "error": "ç”ŸæˆMarkdownæŠ¥å‘Šå¤±è´¥"
            }

    # å‡†å¤‡è¿”å›ç»“æœ
    result = {
        "success": True,
        "evaluation_result": evaluation_result,
        "files": {
            "report_md": report_file,
            "report_json": report_file.replace("evaluation_markdown", "evaluation_json").replace(".md", ".json")
        }
    }
    
    # æ·»åŠ ç›¸åº”çš„æŠ¥å‘Šåˆ°ç»“æœä¸­
    if markdown_report:
        result["markdown_report"] = markdown_report
        result["report"] = markdown_report
        log(f"å·²æ·»åŠ æ ‡å‡†æŠ¥å‘Šåˆ°ç»“æœï¼Œé•¿åº¦: {len(markdown_report)}", important=True)
        
    if markdown_report_iteration:
        result["report_iteration"] = markdown_report_iteration
        log(f"å·²æ·»åŠ è¿­ä»£æŠ¥å‘Šåˆ°ç»“æœï¼Œé•¿åº¦: {len(markdown_report_iteration)}", important=True)
    elif is_iteration:
        log("è¿­ä»£æ¨¡å¼å·²å¯ç”¨ä½†è¿­ä»£æŠ¥å‘Šä¸ºç©ºï¼Œæœªèƒ½æ·»åŠ è¿­ä»£æŠ¥å‘Š", level="WARNING")
    
    # è®°å½•æœ€ç»ˆè¿”å›çš„å­—æ®µ
    log(f"æœ€ç»ˆç»“æœåŒ…å«ä»¥ä¸‹å­—æ®µ: {', '.join(result.keys())}", important=True)
    
    # å¦‚æœæ˜¯è¿­ä»£æ¨¡å¼ï¼Œä¿å­˜ä¸¤ç§æŠ¥å‘Š
    try:
        if is_iteration:
            # ä¿å­˜å®Œæ•´çš„æ ‡å‡†æŠ¥å‘Š
            if markdown_report:
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(report_file), exist_ok=True)
                with open(report_file, 'w', encoding='utf-8', errors='ignore') as f:
                    f.write(markdown_report)
                # éªŒè¯æ–‡ä»¶å†™å…¥æˆåŠŸ
                if os.path.exists(report_file) and os.path.getsize(report_file) > 0:
                    log(f"å®Œæ•´çš„æ ‡å‡†è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file} (å¤§å°: {os.path.getsize(report_file)}å­—èŠ‚)", important=True)
                else:
                    log(f"è­¦å‘Šï¼šæ–‡ä»¶å†™å…¥å¯èƒ½å¤±è´¥ï¼Œæ–‡ä»¶å¤§å°ä¸º0æˆ–æ–‡ä»¶ä¸å­˜åœ¨: {report_file}", level="WARNING")
            
            # ä¿å­˜ç®€æ´çš„è¿­ä»£æŠ¥å‘Š
            if markdown_report_iteration:
                iteration_report_file = report_file.replace(".md", "_iteration.md")
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(iteration_report_file), exist_ok=True)
                with open(iteration_report_file, 'w', encoding='utf-8', errors='ignore') as f:
                    f.write(markdown_report_iteration)
                # éªŒè¯æ–‡ä»¶å†™å…¥æˆåŠŸ
                if os.path.exists(iteration_report_file) and os.path.getsize(iteration_report_file) > 0:
                    log(f"ç®€æ´çš„è¿­ä»£è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ° {iteration_report_file} (å¤§å°: {os.path.getsize(iteration_report_file)}å­—èŠ‚)", important=True)
                else:
                    log(f"è­¦å‘Šï¼šæ–‡ä»¶å†™å…¥å¯èƒ½å¤±è´¥ï¼Œæ–‡ä»¶å¤§å°ä¸º0æˆ–æ–‡ä»¶ä¸å­˜åœ¨: {iteration_report_file}", level="WARNING")
                result["files"]["report_iteration_md"] = iteration_report_file
        else:
            # éè¿­ä»£æ¨¡å¼ï¼Œåªä¿å­˜ä¸€ç§æŠ¥å‘Š
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(report_file), exist_ok=True)
            with open(report_file, 'w', encoding='utf-8', errors='ignore') as f:
                f.write(markdown_report)
            # éªŒè¯æ–‡ä»¶å†™å…¥æˆåŠŸ
            if os.path.exists(report_file) and os.path.getsize(report_file) > 0:
                log(f"Markdownæ ¼å¼çš„è¯„æµ‹æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_file} (å¤§å°: {os.path.getsize(report_file)}å­—èŠ‚)", important=True)
            else:
                log(f"è­¦å‘Šï¼šæ–‡ä»¶å†™å…¥å¯èƒ½å¤±è´¥ï¼Œæ–‡ä»¶å¤§å°ä¸º0æˆ–æ–‡ä»¶ä¸å­˜åœ¨: {report_file}", level="WARNING")
            
        # ä¿å­˜è¯„æµ‹ç»“æœJSONæ–‡ä»¶
        json_file = report_file.replace("evaluation_markdown", "evaluation_json").replace(".md", ".json")
        try:
            with open(json_file, 'w', encoding='utf-8', errors='ignore') as f:
                json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
            log(f"JSONæ ¼å¼çš„è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ° {json_file}", important=True)
        except Exception as e:
            log_error(f"ä¿å­˜JSONæ ¼å¼çš„è¯„æµ‹ç»“æœåˆ° {json_file} å¤±è´¥: {str(e)}")
            
        # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
        await asyncio.sleep(0.1)
    except Exception as e:
        log_error(f"ä¿å­˜Markdownæ ¼å¼çš„è¯„æµ‹æŠ¥å‘Šåˆ° {report_file} å¤±è´¥: {str(e)}")
        import traceback
        log_error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        # ç»§ç»­æ‰§è¡Œï¼Œä¸ä¸­æ–­æµç¨‹

    log("æŠ¥å‘Šç”Ÿæˆæµç¨‹å®Œæˆï¼", important=True)
    # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿æ—¥å¿—é¡ºåº
    await asyncio.sleep(0.1)
    end_logging()

    return result

async def evaluate_with_committee(session: aiohttp.ClientSession,
                                  ai_cases: Dict,
                                  golden_cases: Dict,
                                  duplicate_info_text: str = "",
                                  use_collab_eval: bool = None) -> Dict:
    """
    ä½¿ç”¨è¯„å§”å§”å‘˜ä¼šè¯„æµ‹æµ‹è¯•ç”¨ä¾‹

    :param session: aiohttpä¼šè¯
    :param ai_cases: AIç”Ÿæˆçš„æµ‹è¯•ç”¨ä¾‹
    :param golden_cases: é»„é‡‘æ ‡å‡†æµ‹è¯•ç”¨ä¾‹
    :param duplicate_info_text: é‡å¤æµ‹è¯•ç”¨ä¾‹åˆ†æä¿¡æ¯
    :param use_collab_eval: æ˜¯å¦ä½¿ç”¨CollabEvalæ¡†æ¶ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
    :return: æ±‡æ€»åçš„è¯„æµ‹ç»“æœ
    """
    try:
        # ç›´æ¥ä½¿ç”¨å§”å‘˜ä¼šæ¨¡å—ä¸­çš„å‡½æ•°
        from committee import evaluate_with_committee as committee_evaluate
        return await committee_evaluate(session, ai_cases, golden_cases, duplicate_info_text, use_collab_eval)
    except TypeError as e:
        # å¤„ç†å‚æ•°ä¸åŒ¹é…çš„æƒ…å†µ
        log_error(f"è°ƒç”¨å§”å‘˜ä¼šè¯„æµ‹å‡½æ•°å‡ºç°å‚æ•°ä¸åŒ¹é…: {str(e)}", important=True)
        log("å°è¯•ä¸å¸¦use_collab_evalå‚æ•°è°ƒç”¨", important=True)
        try:
            # å°è¯•ä½¿ç”¨æ—§ç‰ˆæœ¬APIè°ƒç”¨
            from committee import evaluate_with_committee as committee_evaluate
            return await committee_evaluate(session, ai_cases, golden_cases, duplicate_info_text)
        except Exception as e2:
            log_error(f"å¤‡ç”¨è°ƒç”¨ä¹Ÿå¤±è´¥: {str(e2)}", important=True)
            return None
    except Exception as e:
        log_error(f"è¯„å§”å§”å‘˜ä¼šè¯„æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", important=True)
        import traceback
        log_error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return None
